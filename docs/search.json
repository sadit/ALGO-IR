[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Curso Introductorio al Análisis de Algoritmos con Julia",
    "section": "",
    "text": "Prefacio\nEl Análisis de algoritmos es una disciplina formativa enfocada en el desempeño de los algoritmos bajo una cierta entrada. Su estudio nos permite identificar el problema algorítmico subyacente dentro de problemas reales, y por tanto, ser capaces de seleccionar, adaptar o construir una solución eficiente y eficaz para dicho problema. Una solución adecuada sobre una ingenua nos permite mejorar de manera significativa los recursos computacionales, que pueden llevar a reducción de costos de operación en un sistema o la posibilidad de procesar grandes cantidades de información de manera más eficiente.\nEl diseño, implementación y análisis de algoritmos es fundamental para formar el criterio del científico de datos. Los conocimientos adquiridos servirán para obtener las herramientas y la intuición necesaria para plantear la solución a un problema basado en un modelo de cómputo y resolverlo de manera eficiente y escalable cuando sea posible.\nA lo largo de los temas se abordarán los algoritmos y estructuras de manera teórica y práctica, y se motivará al estudiante a realizar sus propias implementaciones. Al terminar este curso, se pretende que el alumno sea competente para seleccionar, diseñar, implementar y analizar algoritmos sobre secuencias, conjuntos y estructuras de datos para resolver problemas optimizando los recursos disponibles, en particular, memoria y tiempo de cómputo. Durante el curso se estudiaran problemas y algoritmos simples, que suelen formar parte de algoritmos más complejos, y por lo tanto, si somos capaces de seleccionar adecuadamente estos bloques más simples, afectaremos directamente el desempeño de los sistemas.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#contenido-del-libro",
    "href": "index.html#contenido-del-libro",
    "title": "Curso Introductorio al Análisis de Algoritmos con Julia",
    "section": "Contenido del libro",
    "text": "Contenido del libro\nEste libro esta diseñado para ser impartido en un semestre de licenciatura o maestría con un enfoque experimental, de Ingeniería en Computación o Ciencias de la Computación, así como Ciencia de Datos. Los algoritmos que se van develando desentrañan los algoritmos clásicos de Recuperación de Información, algoritmos detrás de grandes máquinas de búsqueda, sistemas de información basados en similitud, retrieval augmented generation (RAG), así como de los métodos detrás de la aceleración de otras técnicas de análisis de datos como agrupamiento y reducción de dimensión no-lineal.\n\nEl Cap. 1  Julia como lenguaje de programación para un curso de algoritmos se dedica a revisar el lenguaje de programación Julia, desde un punto de vista de alguien que podría no conocer el lenguaje, pero que definitivamente sabe programar y esta familiarizado con los conceptos generales de un lenguaje de programación moderno.\nEl Cap. 2  Introducción al análisis de algoritmos con Julia introduce los conceptos de análisis asintótico y compara ordenes de crecimiento con la idea de formar intuición.\nEn el Cap. 3  Estructuras de datos elementales nos encontramos con las estructuras de datos elementales como son las estructuras de datos lineales y de acceso aleatorio, y su organización en memoria.\nEl Cap. 4  Algoritmos de ordenamiento esta dedicado a algoritmos de ordenamiento en el modelo de comparación, estudia algoritmos tanto de peor caso como aquellos que toman ventaja de la distribución de entrada.\nEn el Cap. 5  Algoritmos de búsqueda en el modelo de comparación abordamos algoritmos de búsqueda en arreglos ordenados en el modelo de comparación. De nueva cuenta se abordan algoritmos de peor caso y algoritmos que pueden sacar ventaja de instancias fáciles.\nFinalmente, el Cap. 6  Algoritmos de intersección y unión de conjuntos en el modelo de comparación estudia algoritmos de intersección de conjuntos, los cuales son la base de sistemas de información capaces de manipular cantidades enormes de datos.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#trabajo-en-progreso",
    "href": "index.html#trabajo-en-progreso",
    "title": "Curso Introductorio al Análisis de Algoritmos con Julia",
    "section": "Trabajo en progreso",
    "text": "Trabajo en progreso\nEste libro es un trabajo en progreso, que se pretende términar durante el primer semestre de 2025, mientras se imparte el curso Análisis de algoritmos en la Maestría en Ciencia de Datos e Información de INFOTEC, México. El perfil de ingreso de la maestría es multidisciplinario, y esto es parte esencial del diseño de este libro.\nEn particular, los capítulos 1, 2, y 3 tienen un avance significativo, aunque no estan terminados. El resto de los capítulos ese encuentran en un estado incipiente.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#licencia",
    "href": "index.html#licencia",
    "title": "Curso Introductorio al Análisis de Algoritmos con Julia",
    "section": "Licencia",
    "text": "Licencia\n\nEsta obra está bajo una Licencia Creative Commons Atribución-CompartirIgual 4.0 Internacional",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "cap1-julia.html",
    "href": "cap1-julia.html",
    "title": "1  Julia como lenguaje de programación para un curso de algoritmos",
    "section": "",
    "text": "1.1 El lenguaje de programación Julia\nNuestro objetivo trabajar sobre algoritmos, por lo que cualquier lenguaje que pueda expresar todo lo computable, puede ser adecuado. Pero dado que nuestro enfoque será experimental, y nuestra metodología incluye medir la factibilidad y desempeño de cada algoritmo en términos reales, entonces necesitamos un lenguaje donde las instrucciones, los acceso a memoria, y la manipulación de la misma sea controlable. En este caso, y mediando con la fácilidad de aprendizaje y la productividad, este curso utiliza el lenguaje de programación Julia.1 Pero no hay porque preocuparse por aprender un nuevo lenguaje, el curso utiliza ejemplos en Julia y utiliza una variante de su sintaxis como pseudo-código, pero las actividades se esperan tanto en Julia como en Python.\nAmbos lenguajes de programación son fáciles de aprender y altamente productivos. Python es un lenguaje excelente para realizar prototipos, o para cuando existen bibliotecas que resuelvan el problema que se este enfrentando. Por otro lado, cuando se necesita control sobre las operaciones que se estan ejecutando, o la memoria que se aloja, Python no es un lenguaje que nos permita trabajar en ese sentido. Julia esta diseñado para ser veloz y a la vez mantener el dinámismo que se espera de un lenguaje moderno, adicionalmente, es posible conocer los tipos de instrucciones que realmente se ejecutan, así como también es posible controlar la alojación de memoria, ya se mediante la utilización de patrones que así nos lo permitan, o mediante instrucciones que nos lo aseguren.\nEste curso esta escrito en Quarto, y se esperan reportes de de tareas y actividades tanto en Quarto https://quarto.org como en Jupyter https://jupyter.org/. La mayoría de los ejemplos estarán empotrados en el sitio, y en principio, deberían poder replicarse copiando, pegando, y ejecutando en una terminal de Julia.\nEs importante clarificar que este capítulo introducirá el lenguaje de programación Julia hasta el nivel que se requiere en este curso, ignorando una gran cantidad de capacidades que no son de interés para nuestro curso. Se recomienda al alumno interesado la revisión del manual y la documentación oficial para un estudio más profundo del lenguaje.\nJulia es un lenguaje singular, es un lenguaje dinámico y de alto nivel, tiene de tipado fuerte y compila a código máquina para cada una de las instrucciones que se dan. Su interfaz más común es un REPL o , esto es que puede ser utilizado de manera interactiva, además de la ejecución en scripts o notebooks como los que estaremos usando para reportar.\nEs homoicónico, que significa que la manera en que se representan sus programas coincide con las estructuras de datos básicas, lo cual permite crear programas validos mediante programas. De manera práctica, también le permite la reescritura de los programas utilizando otro programa utilizando macros, los cuales son funciones que modifican el código y empiezan con el simbolo @. Estaremos viendo una serie de macros con propósitos muy específicos, crear macros y la manipulación automática de código cae fuera de nuestro curso.\nEl lenguaje tiene estructuras de datos básicas como rangos, vistas, tuplas, arreglos, estructuras, diccionarios, conjuntos, cadenas de caracteres, así como expresiones de código como datos y controla la ejecución mediante condicionales, ciclos y funciones. Tiene un sistema de tipos de datos muy poderoso, que le permite entre otras cosas generar código específico para dichos tipos. El código se organiza en scripts, y a nivel lógico en módulos y paquetes. Una de sus características importantes el despacho múltiple en las funciones, esto es, que para cada conjunto de tipos de argumentos, compilará una función especializada. Este patrón puede ser muy poderoso para escribir código genérico que pueda ser muy eficiente, a costa de múltiples códigos de máquina para una función. Esta estrategía también viene con el problema que la primera vez que se ejecuta una función con un conjunto específico de tipos de argumentos, dicha función será especializada y compilada, lo cual puede representar un costo inicial importante en algunos casos donde no se pretenda procesar grandes cantidades de información. En particular, este problema se ha venido reduciendo en las versiones más nuevas de Julia haciendo uso una estrategía de precompilación para datos típicos.\nEntre los tipos de datos es capaz de manera enteros y números de punto flotante de diferentes precisiones, caracteres, cadenas de caracteres, y simbolos. Los arreglos son realmente importantes en Julia, y soportan de manera nativa vectores, matrices y tensores, estaremos tocando apenas esta parte del lenguaje. El resto de esta unidad esta dedicada a precisar la sintaxis del lenguaje y anotaciones de importancia sobre su funcionamiento, y en particular, en el manejo que nos permitirá generar código eficiente que limite el alojamiento de memoria.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Julia como lenguaje de programación para un curso de algoritmos</span>"
    ]
  },
  {
    "objectID": "cap1-julia.html#el-lenguaje-de-programación-julia",
    "href": "cap1-julia.html#el-lenguaje-de-programación-julia",
    "title": "1  Julia como lenguaje de programación para un curso de algoritmos",
    "section": "",
    "text": "1.1.1 Funciones\nLas funciones son centrales en Julia, y son definidas mediante la sintaxis\n```{julia}\n1function fun(arg1, arg2...)\n    # ... expresiones ...\nend\n\n2function fun(arg1, arg2...; kwarg1=valor1, kwargs2...)\n    # ... expresiones ...\nend\n\n3fun(arg1, arg2...; kwarg1=valor1, kwargs2...) = expresion\n\n4(arg1, arg2...; kwarg1=valor1, kwargs2...) -&gt; expresion\n\n5fun() do x\n    x^2 # ... expresiones ...\nend\n```\n\n1\n\nDefinición de una función simple, los tipos de los argumentos se utilizan para generar múltiples versiones de una función.\n\n2\n\nTambién se soportan argumentos nombrados, los cuales van después de ;, se debe tener en cuenta que los tipos de los argumentos nombrados no son utilizados para determinar si una función debe compilarse. Los argumentos nombrados pueden o no tener valores por omisión.\n\n3\n\nSi la función tiene una estructura simple, de una expresión, es posible ignorar function y end, usando ‘=’ para definirla.\n\n4\n\nMuchas veces es útil definir funciones anónimas, que suelen pasarse a otras funciones de orden superior.\n\n5\n\nUn embellecedor útil para generar una función anónima (definida entre do...end) que se pasa como primer argumento a fun, e.g., es equivalente a fun(x-&gt;x^2).\n\n\nEl ámbito o scope de las variables en Julia es sintáctico, que significa que se hereda del código donde las funciones fueron definidas, y no dinámico (que se hereda desde dónde se ejecuta la función). Aunque es el comportamiento de la mayoría de los lenguajes modernos, es importante conocerlo sobre todo para la creación de cerraduras sintácticas en funciones.\nUna función se ejecuta con la sintaxis nombre(arg1...). Conviene profundizar en las expresiones y demás componentes del lenguaje antes del ir a más ejemplos sobre funciones.\n\n\n1.1.2 Hola mundo\nUno de los programas más comunes es el siguiente\n\nprintln(\"¡Hola 🌎!\")\n\n¡Hola 🌎!\n\n\n\n\n1.1.3 Expresiones y operadores\nLas expresiones son la forma más genérica de expresar el código en Julia, comprenden operaciones aritméticas, asignación y declaración de variables, definiciones de bloques de código, llamadas de funciones, entre otras.\nCada linea suele ser una expresión, a menos que se extienda por múltiples lineas por medio de un agrupador de código o datos, estos pueden ser begin...end, let...end, (...), [...], [...], for...end, while...end, if...end, function...end, try...end, entre las más utilizadas.\nLas definiciones de variables tienen la sintaxis variable = valor; las variables comunmente comienzan con una letra o _, las letras pueden ser caracteres unicode, no deben contener espacios ni puntuaciones como parte del nombre; valor es el resultado de evaluar o ejecutar una expresión.\nLos operadores más comunes son los aritméticos +, -, *, /, ÷, %, \\, ^, con precedencia y significado típico. Existen maneras compuestas de modificar una variable anteponiendo el operador aritmético al simbolo de asignación, e.g., variable += valor, que se expande a variable = variable + valor. Esto implica que variable debe estar previamente definida previo a la ejecución.\nLos operadores lógicos también tienen el significado esperado.\n\n\n\noperación\ndescripción\n\n\n\n\na && b\nAND lógico\n\n\na || b\nOR lógico\n\n\na ⊻ b\nXOR lógico\n\n\n!a\nnegación lógica\n\n\na &lt; b\ncomparación a es menor que b\n\n\na &gt; b\ncomparación a es mayor que b\n\n\na &lt;= b\ncomparación a es menor o igual que b\n\n\na &gt;= b\ncomparación a es mayor o igual que b\n\n\na == b\ncomparación de igualdad\n\n\na === b\ncomparación de igualdad (a nivel de tipo)\n\n\na != b\ncomparación de desigualdad\n\n\na !== b\ncomparación de desigualdad (a nivel de tipo)\n\n\n\nEn particular && y || implementan corto circuito de código, por lo que pueden usarse para el control de que operaciones se ejecutan. Cuando se compara a nivel de tipo 0 (entero) será diferente de 0.0 (real).\nTambién hay operadores lógicos a nivel de bit, los argumentos son enteros.\n\n\n\noperación\ndescripción\n\n\n\n\na & b\nAND a nivel de bits\n\n\na | b\nOR a nivel de bits\n\n\na ⊻ b\nXOR a nivel del bits\n\n\n~a\nnegación lógica a nivel de bits\n\n\n\n\n\n1.1.4 Literales\nDado que existen múltiples tipos de datos existen diferentes formas de definirlas; una de ellas, probablemente la que más estaremos usando son los literales, es decir, escribir los datos directamente en el código.\nLos números enteros se definen sin punto decimal, es posible usar _ como separador y dar más claridad al código. Los enteros pueden tener 8, 16, 32, o 64 bits; por omisión, se empaquetan en variables del tipo Int (Int64). Los valores hexadecimales se interpretan como enteros sin signo, y además se empaquetan al número de bits necesario minimo para contener. El comportamiento para valores en base 10 es el de hexadecimal es congruente con un lenguaje para programación de sistemas.\n\na = 100\nprintln((a, sizeof(a)))\nb = Int8(100)\nprintln((b, sizeof(b)))\nc = 30_000_000\nprintln((c, sizeof(c)))\nd = 0xffff\nprintln((d, sizeof(d)))\n\n(100, 8)\n(100, 1)\n(30000000, 8)\n(0xffff, 2)\n\n\n\n\nExisten números enteros de precisión 128 pero las operaciones al día de hoy no son implementadas de manera nativa por los procesadores; así mismo se reconocen números de punto flotante de precisión media Float16 pero la mayoría de los procesadores no tienen soporte nativo para realizar operaciones con ellos, aunque los procesadores de última generación si lo tienen.\nSi la precisión esta en duda o el contexto lo amérita, deberá especificarlo usando el constructor del tipo e.g., Int8(100), UInt8(100), Int16(100), UInt16(100), Int32(100), UInt32(100), Int64(100), UInt64(100).\nLos números de punto flotante tienen diferentes formas de definirse, teniendo diferentes efectos. Para números de precision simple, 32 bits, se definen con el sufijo f0 como 3f0. El sufijo e0 también se puede usar para definir precisión doble (64 bit). El cero del sufijo en realidad tiene el objetivo de colocar el punto decimal, en notación de ingeniería, e.g., \\(0.003\\) se define como \\(3f-3\\) o \\(3e-3\\), dependiendo del tipo de dato que se necesite. Si se omite sufijo y se pone solo punto decimal entonces se interpretará como precision doble. Los tipos son Float32 y Float64.\nLos datos booleanos se indican mediante true y false para verdadero y falso, respectivamente.\nLos caracteres son símbolos para índicar cadenas, se suelen representar como enteros pequeños en memoria. Se especifican con comillas simples 'a', 'z', '!' y soporta simbolos unicode '🤠'.\nLas cadenas de caracteres son la manera de representar textos como datos, se guardan en zonas contiguas de memoria. Se especifican con comillas dobles y también soportan símbolos unicode, e.g., \"hola mundo\", \"pato es un 🐷\".\n\n\nJulia guarda los simbolos de manera especial y pueden ser utilizados para realizar identificación de datos eficiente, sin embargo, no es buena idea saturar el sistema de manejo de símbolos por ejemplo para crear un vocabulario ya que no liberará la memoria después de definirlos ya que es un mecánismo diseñado para la representación de los programas, pero lo suficientemente robusto y bien definido para usarse en el diseño e implementación de programas de los usuarios.\nEn Julia existe la noción de símbolo, que es una cadena que además solo existe en una posición en memoria se usa el prefijo : para denotarlos.\n\nprintln(:hola === :hola)\nprintln(typeof(:hola))\nprintln(Symbol(\"hola mundo\"))\n\ntrue\nSymbol\nhola mundo\n\n\n\n\n1.1.5 Control de flujo\nEl control de flujo nos permite escoger que partes del código se ejecutaran como consecuencia de la evaluación de una expresión, esto incluye repeticiones.\nLas condicionales son el control de flujo más simple.\n\na = 10\n1if a % 2 == 0\n2    \"par\"\nelse\n3    \"impar\"\nend\n\n\n1\n\nExpresión condicional.\n\n2\n\nExpresión a ejecutarse si (1) es verdadero.\n\n3\n\nExpresión a evaluarse si (1) es falso.\n\n\n\n\n\"par\"\n\n\nSe puede ignorar la clausula else dando solo la opción de evaluar (2) si (1) es verdadero. Finalmente, note que la condicional es una expresión y devuelve un valor.\n\na = 10\nif log10(a) == 1\n    \"es 10\"\nend\n\n\"es 10\"\n\n\nTambién pueden concatenarse múltiples expresiones condicionales con elseif como se muestra a continuación.\n\na = 9\nif a % 2 == 0\n    println(\"divisible entre 2\")\nelseif a % 3 == 0\n    println(\"divisible entre 3\")\nelse\n    println(\"no divisible entre 2 y 3\")\nend\n\ndivisible entre 3\n\n\nEs común utilizar la sintaxis en Julia (short circuit) para control de flujo:\n\na = 9\n\n1println(a % 2 == 0 && \"es divisible entre dos\")\n2println(a % 3 == 0 && \"es divisible entre tres\")\n\n\n1\n\nEl resultado de la condición es falso, por lo que no se ejecutará la siguiente expresión.\n\n2\n\nEl resultado es verdadero, por lo que se ejecutará la segunda expresión.\n\n\n\n\nfalse\nes divisible entre tres\n\n\nFnalmente, existe una condicional de tres vias expresion ? expr-verdadero : expr-falso\n\na = 9\n\nprintln(a % 2 == 0 ? \"es divisible entre dos\" : \"no es divisible entre dos\")\nprintln(a % 3 == 0 ? \"es divisible entre tres\" : \"no es divisible entre tres\")\n\nno es divisible entre dos\nes divisible entre tres\n\n\n\n1.1.5.1 Ciclos\nLos ciclos son expresiones de control de flujo que nos permiten iterar sobre una colección o repetir un código hasta que se cumpla alguna condición. En Julia existen dos expresiones de ciclos:\n\nfor x in colección ...expresiones... end y\nwhile condición ...expresioens... end\n\nEn el caso de for, la idea es iterar sobre una colección, esta colección puede ser un rango, i.e., inicio:fin, inicio:paso:fin, o una colección como las tuplas, los arreglos, o cualquiera que cumpla con la interfaz de colección iterable del lenguaje.\n\nfor i in 1:5\n    println(\"1er ciclo: \", i =&gt; i^2)\nend\n\nfor i in [10, 20, 30, 40, 50]\n    println(\"2do ciclo: \", i =&gt; i/10)\nend\n\n1er ciclo: 1 =&gt; 1\n1er ciclo: 2 =&gt; 4\n1er ciclo: 3 =&gt; 9\n1er ciclo: 4 =&gt; 16\n1er ciclo: 5 =&gt; 25\n2do ciclo: 10 =&gt; 1.0\n2do ciclo: 20 =&gt; 2.0\n2do ciclo: 30 =&gt; 3.0\n2do ciclo: 40 =&gt; 4.0\n2do ciclo: 50 =&gt; 5.0\n\n\nAl igual que en otros lenguajes modernos, se define la variante completa o comprehensive for que se utiliza para transformar la colección de entrada en otra colección cuya sintaxis se ejemplifica a continuación:\n\na = [i =&gt; i^2 for i in 1:5]\nprintln(a)\n\n[1 =&gt; 1, 2 =&gt; 4, 3 =&gt; 9, 4 =&gt; 16, 5 =&gt; 25]\n\n\nTambién es posible definir un generador, esto es, un código que puede generar los datos, pero que no los generará hasta que se les solicite.\n\na = (i =&gt; i^2 for i in 1:5)\nprintln(a)\nprintln(collect(a))\n\nBase.Generator{UnitRange{Int64}, var\"#3#4\"}(var\"#3#4\"(), 1:5)\n[1 =&gt; 1, 2 =&gt; 4, 3 =&gt; 9, 4 =&gt; 16, 5 =&gt; 25]\n\n\nOtra forma de hacer ciclos de intrucciones es repetir mientras se cumpla una condición:\n\ni = 0\nwhile i &lt; 5\n    i += 1\n    println(i)\nend\n\ni\n\n1\n2\n3\n4\n5\n\n\n5\n\n\n\n\n\n1.1.6 Tuplas y arreglos en Julia\nUna tupla es un conjunto ordenado de datos que no se puede modificar y que se desea esten contiguos en memoria, la sintaxis en memoria es como sigue:\n\n1a = (2, 3, 5, 7)\nb = (10, 20.0, 30f0)\nc = 100 =&gt; 200\n2println(typeof(a))\nprintln(typeof(b))\nprintln(typeof(c))\n3a[1], a[end], b[3], c.first, c.second\n\n\n1\n\nDefine las tuplas.\n\n2\n\nImprime los tipos de las tuplas.\n\n3\n\nMuestra como se accede a los elementos de las tuplas. Julia indexa comenzando desde 1, y el término end también se utiliza para indicar el último elemento en una colección ordenada.\n\n\n\n\nNTuple{4, Int64}\nTuple{Int64, Float64, Float32}\nPair{Int64, Int64}\n\n\n(2, 7, 30.0f0, 100, 200)\n\n\nLa misma sintaxis puede generar diferentes tipos de tuplas. En el caso NTuple{4, Int4} nos indica que el tipo maneja cuatro elementos de enteros de 64 bits, los argumentos entre {} son parametros que especifican los tipos en cuestión. En el caso de Tuple se pueden tener diferentes tipos de elementos. La tupla Pair es especial ya que solo puede contener dos elementos y es básicamente para embellecer o simplificar las expresiones; incluso se crea con la sintaxis key =&gt; value y sus elementos pueden accederse mediante dos campos nombrados.\nLos arreglos son datos del mismo tipo contiguos en memoria, a diferencia de las tuplas, los elementos se pueden modificar, incluso pueden crecer o reducirse. Esto puede implicar que se alojan en zonas de memoria diferente (las tuplas se colocan en el stack y los arreglos en el heap, ver la siguiente unidad para más información). Desde un alto nivel, los arreglos en Julia suelen estar asociados con vectores, matrices y tensores, y un arsenal de funciones relacionadas se encuentran definidas en el paquete LinearAlgebra, lo cual esta más allá del alcance de este curso.\n\n1a = [2, 3, 5, 7]\nb = [10, 20.0, 30f0]\n2println(typeof(a))\nprintln(typeof(b))\n3a[1], a[end], b[3], b[2:3]\n\n\n1\n\nDefine los arreglos a y b.\n\n2\n\nMuestra los tipos de los arreglos, note como los tipos se promueven al tipo más génerico que contiene la definición de los datos.\n\n3\n\nEl acceso es muy similar a las tuplas para arreglos unidimensionales, note que es posible acceder rangos de elementos con la sintaxis ini:fin.\n\n\n\n\nVector{Int64}\nVector{Float64}\n\n\n(2, 7, 30.0, [20.0, 30.0])\n\n\n\na = [2 3;\n1     5 7]\n2display(a)\n3display(a[:, 1])\n4display(a[1, :])\n\n\n1\n\nDefinición de un arreglo bidimensional, note como se ignora la coma , en favor de la escritura por filas separadas por ;.\n\n2\n\nLa variable a es una matriz de 2x2.\n\n3\n\nEs posible acceder una columna completa usando el símbolo : para indicar todos los elementos.\n\n4\n\nDe igual forma, es posible acceder una fila completa.\n\n\n\n\n2×2 Matrix{Int64}:\n 2  3\n 5  7\n\n\n2-element Vector{Int64}:\n 2\n 5\n\n\n2-element Vector{Int64}:\n 2\n 3\n\n\n\n\n1.1.7 Diccionarios y conjuntos en Julia\nUn diccionario es un arreglo asociativo, i.e., guarda pares llave-valor. Permite acceder de manera eficiciente al valor por medio de la llave, así como también verificar si hay una entrada dentro del diccionario con una llave dada. La sintaxis es como sigue:\n\n1a = Dict(:a =&gt; 1, :b =&gt; 2, :c =&gt; 3)\n2a[:b] = 20\nprintln(a)\n3a[:d] = 4\nprintln(a)\n4delete!(a, :a)\na\n\n\n1\n\nDefinición del diccionario a que mapea simbolos a enteros.\n\n2\n\nCambia el valor de :b por 20.\n\n3\n\nAñade :d =&gt; 4 al diccionario a.\n\n4\n\nBorra el par con llave :a.\n\n\n\n\nDict(:a =&gt; 1, :b =&gt; 20, :c =&gt; 3)\nDict(:a =&gt; 1, :b =&gt; 20, :d =&gt; 4, :c =&gt; 3)\n\n\nDict{Symbol, Int64} with 3 entries:\n  :b =&gt; 20\n  :d =&gt; 4\n  :c =&gt; 3\n\n\nEs posible utilizar diferentes tipos siempre y cuando el tipo en cuestión defina de manera correcta la función hash sobre la llave y la verificación de igualdad ==.\nUn conjunto se representa con el tipo Set, se implementa de manera muy similar al diccionario pero solo necesita el elemento (e.g., la llave). Como conjunto implementa las operaciones clasificación de operaciones de conjuntos\n\n1a = Set([10, 20, 30, 40])\n2println(20 in a)\n3push!(a, 50)\nprintln(a)\n4delete!(a, 10)\nprintln(a)\n5println(intersect(a, [20, 35]))\n6union!(a, [100, 200])\nprintln(a)\n\n\n1\n\nDefinición del conjunto de números enteros.\n\n2\n\nVerificación de membresia al conjunto a.\n\n3\n\nAñade 50 al conjunto.\n\n4\n\nSe borra el elemento 10 del conjunto.\n\n5\n\nIntersección de a con una colección, no se modifica el conjunto a.\n\n6\n\nUnión con otra colección, se modifica a.\n\n\n\n\ntrue\nSet([50, 20, 10, 30, 40])\nSet([50, 20, 30, 40])\nSet([20])\nSet([50, 200, 20, 30, 40, 100])",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Julia como lenguaje de programación para un curso de algoritmos</span>"
    ]
  },
  {
    "objectID": "cap1-julia.html#el-flujo-de-compilación-de-julia",
    "href": "cap1-julia.html#el-flujo-de-compilación-de-julia",
    "title": "1  Julia como lenguaje de programación para un curso de algoritmos",
    "section": "1.2 El flujo de compilación de Julia",
    "text": "1.2 El flujo de compilación de Julia\nBasta con escribir una linea de código en el REPL de Julia y esta se compilará y ejecutará en el contexto actual, usando el ámbito de variables. Esto es conveniente para comenzar a trabajar, sin embargo, es importante conocer el flujo de compilación para tenerlo en cuenta mientras se códifica, y así generar código eficiente. En particular, la creación de funciones y evitar la inestabilidad de los tipos de las variables es un paso hacia la generación de código eficiente. También es importante evitar el alojamiento de memoria dinámica siempre que sea posible. A continuación se mostrará el análisis de un código simple a diferentes niveles, mostrando que el lenguaje nos permite observar la generación de código, que últimadamente nos da cierto control y nos permite verificar que lo que se esta implementando es lo que se específica en el código. Esto no es posible en lenguajes como Python.\n\nlet\n    e = 1.1\n    println(e*e)\n    @code_typed e*e\nend\n\n1.2100000000000002\n\n\nCodeInfo(\n1 ─ %1 = Base.mul_float(x, y)::Float64\n└──      return %1\n) =&gt; Float64\n\n\nEn este código, se utiliza la estructa de agrupación de expresiones let...end. Cada expresión puede estar compuesta de otras expresiones, y casi todo es una expresión en Julia. La mayoria de las expresiones serán finalizadas por un salto de linea, pero las compuestas como let, begin, function, if, while, for, do, module estarán finalizadas con end. La indentación no importa la indentación como en Python, pero es aconsejable para mantener la legibilidad del código. La linea 2 define e inicializa la variable e; la linea 3 llama a la función println, que imprimirá el resultado de e*e en la consola. La función println esta dentro de la biblioteca estándar de Julia y siempre esta visible. La linea 4 es un tanto diferente, es una macro que toma la expresión e*e y realiza algo sobre la expresión misma, en particular @code_type muestra como se reescribe la expresión para ser ejecutada. Note como se hará una llamada a la función Base.mul_float que recibe dos argumentos y que regresará un valor Float64. Esta información es necesaria para que Julia pueda generar un código veloz, el flujo de compilación llevaría esta información a generar un código intermedio de Low Level Virtual Machine (LLVM), que es el compilador empotrado en Julia, el cual estaría generando el siguiente código LLVM (usando la macro @code_llvm):\n\n\n;  @ float.jl:411 within `*`\ndefine double @\"julia_*_1779\"(double %0, double %1) #0 {\ntop:\n  %2 = fmul double %0, %1\n  ret double %2\n}\n\n\nEste código ya no es específico para Julia, sino para la maquinaría LLVM. Observe la especificidad de los tipos y lo corto del código. El flujo de compilación requeriría generar el código nativo, que puede ser observado a continuación mediante la macro @code_native:\n\n\n    .text\n    .file   \"*\"\n    .globl  \"julia_*_1818\"                  # -- Begin function julia_*_1818\n    .p2align    4, 0x90\n    .type   \"julia_*_1818\",@function\n\"julia_*_1818\":                         # @\"julia_*_1818\"\n; ┌ @ float.jl:411 within `*`\n# %bb.0:                                # %top\n    push    rbp\n    mov rbp, rsp\n    vmulsd  xmm0, xmm0, xmm1\n    pop rbp\n    ret\n.Lfunc_end0:\n    .size   \"julia_*_1818\", .Lfunc_end0-\"julia_*_1818\"\n; └\n                                        # -- End function\n    .section    \".note.GNU-stack\",\"\",@progbits\n\n\nEn este caso podemos observar código específico para la computadora que esta generando este documento, es posible ver el manejo de registros y el uso de instrucciones del CPU en cuestión.\nEste código puede ser eficiente dado que los tipos y las operaciones son conocidos, en el caso que esto no puede ser, la eficiencia esta perdida. Datos no nativos o la imposibilidad de determinar un tipo causarían que se generará más código nativo que terminaría necesitanto más recursos del procesador. Una situación similar ocurre cuando se aloja memoria de manera dinámica. Siempre estaremos buscando que nuestro código pueda determinar el tipo de datos para que el código generado sea simple, si es posible usar datos nativos, además de no manejar o reducir el uso de memoría dinámica.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Julia como lenguaje de programación para un curso de algoritmos</span>"
    ]
  },
  {
    "objectID": "cap1-julia.html#ejemplos-de-funciones",
    "href": "cap1-julia.html#ejemplos-de-funciones",
    "title": "1  Julia como lenguaje de programación para un curso de algoritmos",
    "section": "1.3 Ejemplos de funciones",
    "text": "1.3 Ejemplos de funciones\nLas funciones serán una parte central de nuestros ejemplos, por lo que vale la pena retomarlas y dar ejemplos.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Julia como lenguaje de programación para un curso de algoritmos</span>"
    ]
  },
  {
    "objectID": "cap1-julia.html#recursos-para-aprender-python-y-julia",
    "href": "cap1-julia.html#recursos-para-aprender-python-y-julia",
    "title": "1  Julia como lenguaje de programación para un curso de algoritmos",
    "section": "1.4 Recursos para aprender Python y Julia",
    "text": "1.4 Recursos para aprender Python y Julia\n\n1.4.1 Python\n\nPython, se recomieda utilizar la distribución de https://www.anaconda.com/download/\nDocumentación oficial, comenzar por el tutorial https://docs.python.org/3/\nDocumentación oficial https://docs.julialang.org/en/stable/\n\n\n\n1.4.2 Julia\n\nInformación sobre como instalar Julia y flujos de trabajo simples (e.g., REPL, editores, etc.) para trabajar con este lenguaje de programación: Modern Julia Workflows https://modernjuliaworkflows.github.io/.\nLibro sobre julia Think Julia: How to Think Like a Computer Scientist https://benlauwens.github.io/ThinkJulia.jl/latest/book.html.\nCurso Introduction to computational thinking https://computationalthinking.mit.edu/Fall20/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Julia como lenguaje de programación para un curso de algoritmos</span>"
    ]
  },
  {
    "objectID": "cap1-julia.html#licencia",
    "href": "cap1-julia.html#licencia",
    "title": "1  Julia como lenguaje de programación para un curso de algoritmos",
    "section": "1.5 Licencia",
    "text": "1.5 Licencia\n\nEsta obra está bajo una Licencia Creative Commons Atribución-CompartirIgual 4.0 Internacional",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Julia como lenguaje de programación para un curso de algoritmos</span>"
    ]
  },
  {
    "objectID": "cap1-julia.html#footnotes",
    "href": "cap1-julia.html#footnotes",
    "title": "1  Julia como lenguaje de programación para un curso de algoritmos",
    "section": "",
    "text": "Se recomienda utilizar la versión 1.10 o superior, y puede obtenerse en https://julialang.org/.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Julia como lenguaje de programación para un curso de algoritmos</span>"
    ]
  },
  {
    "objectID": "cap2-analisis.html",
    "href": "cap2-analisis.html",
    "title": "2  Introducción al análisis de algoritmos con Julia",
    "section": "",
    "text": "2.1 Concepto de algoritmo y estructura de datos\nEste capítulo introduce a los fundamentos de análisis de algoritmos. Se introduce el concepto de modelo de cómputo y la notación asintótica, preparandonos para usar el lenguaje común en el análisis de algoritmos. También se mostrarán algunos de los ordenes de crecimiento más representativos, que nos permitirán comparar algoritmos que resuelvan una tarea dada, así como catalogarlos con respecto a los recursos de computo necesarios para ejecutarlos.\nLos algoritmos son especificaciones formales de los pasos u operaciones que deben aplicarse a un conjunto de entradas para resolver un problema, obteniendo una solución correcta a dicho problema. Establecen los fundamentos de la programación y delinean la manera en como se diseñan los programas de computadoras.\nEs común encontrar que un problema puede ser resuelto por múltiples algoritmos, cada uno de ellos con sus diferentes particularidades. Así mismo, un problema suele estar conformado por una cantidad enorme de instancias de dicho problema, por ejemplo, para una lista de \\(n\\) números, existen \\(n!\\) formas de acomodarlos, de tal forma que puedan ser la entrada a un algoritmo cuya entrada sea una lista de números donde el orden es importante. En ocasiones, los problemas pueden tener infinitas de instancias. En este curso nos enfocaremos en problemas que pueden ser simplificados a una cantidad finita instancias.\nCada paso u operación en un algoritmo esta bien definido y puede ser aplicado o ejecutado para producir un resultado. A su vez, cada operación suele tener un costo, dependiente del módelo de computación. Conocer el número de operaciones necesarias para transformar la entrada en la salida esperada, i.e., resolver el problema, es de vital importancia para seleccionar el mejor algoritmo para dicho problema, o aun más, para instancias de dicho problema que cumplen con ciertas características.\nUna estructura de datos es una abstracción en memoria de entidades matemáticas y lógicas que nos permite organizar, almacenar y procesar datos en una computadora. El objetivo es que la información representada puede ser manipulada de manera eficiente en un contexto específico, además de simplificar la aplicación de operaciones para la aplicación de algoritmos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción al análisis de algoritmos con Julia</span>"
    ]
  },
  {
    "objectID": "cap2-analisis.html#modelos-de-cómputo",
    "href": "cap2-analisis.html#modelos-de-cómputo",
    "title": "2  Introducción al análisis de algoritmos con Julia",
    "section": "2.2 Modelos de cómputo",
    "text": "2.2 Modelos de cómputo\nUn modelo de cómputo es una abstracción matemática de una computadora o marco de trabajo algorítmico que nos permite estudiar y medir los costos de los algoritmos funcionando en este modelo de tal forma que sea más simple que una computadora física real. Ejemplos de estos modelos:\n\nLa máquina de Turing.\nFunciones recursivas.\nCálculo lambda.\nMáquina de acceso aleatorio (RAM).\n\nTodas estos modelos son equivalentes en sus capacidades, pero sus diferentes planteamientos permiten enfocarse en diferentes aspectos de los problemas.\n\n2.2.1 Máquina de Turing\nEs un módelo creado por Alan Turing a principios del siglo XX; la idea es un dispositivo que podría ser implementada de manera mecánica si se tuvieran recursos infinitos; esta máquina puede leer y escribir mediante un cabezal en una cinta infinita (ver Figura 2.1) una cantidad de símbolos predeterminada para cada problema siguiendo una serie de reglas simples sobre lo que lee y escribe, dichas reglas y la cienta, forman una máquina de estados y memoria, que pueden realizar cualquier cálculo.\n\n\n\n\n\n\n\n\nTuringTape\n\n\ncluster_tape\n\ncinta\n\n\n\nstart\n\n\n\n\n01\n\n0\n\n\n\n\n02\n\n0\n\n\n\n\n03\n\n0\n\n\n\n\n11\n\n1\n\n\n\n\n12\n\n1\n\n\n\n\n13\n\n1\n\n\n\n\nend\n\n\n\n\n\nhead\n\ncabezal\n\n\n\nhead-&gt;01\n\n\n\n\n\n\n\n\nFigura 2.1: Cabezal y cinta.\n\n\n\n\n\nUna máquina de Turing se puede escribir como una tupla de 7 elementos \\(M=(Q, \\Sigma, \\Gamma, s, \\epsilon, F, \\delta)\\) donde:\n\n\\(Q\\) es un conjunto finito de estados.\n\\(\\Sigma\\) es el alfabeto de entrada, i.e., un conjunto finito de símbolos que no incluye el espacio en blanco.\n\\(\\Gamma\\) es el alfabeto de la cinta , i.e., un conjunto finito de símbolos \\(\\Sigma \\subseteq \\Gamma\\).\n\\(s \\in Q\\) es el estado inicial.\n\\(\\epsilon \\in \\Gamma\\) es un símbolo especial denominado blanco; y puede llenar la cinta al infinito.\n\\(F \\subseteq Q\\) conjunto de estados finales de aceptación.\n\\(\\delta: Q \\times \\Gamma \\rightarrow Q \\times \\Gamma \\times \\{L, R\\}\\), es la función de transición, i.e., una función parcial que índica que se debe hacer al leer un símbolo en la posición actual de lectura, esto es, qué se escribe, hacia que estado se cambia, y la dirección de movimiento de la cinta (siempre se mueve en una celda).\n\nHay muchas variantes de la definición de máquina de Turing, por ejemplo, una cinta especial para lectura y una para escritura, diferentes formas de definir las transiciones, símbolos para no moverse, etc. Hasta ahora, todas las formas son a lo más equivalentes en términos de poder de cómputo, la diferencia viene en la expresividad para definir soluciones.\nEs cómun plantear los problemas en forma de lenguajes; esto es, en términos de dada la cadena S ¿la máquina \\(M\\) la acepta?, dónde acepta quiere decir que la máquina es capaz de ir desde el estado inicial al estado de aceptación leyendo/transformando \\(S\\). Si \\(M\\) no termina en un estado de aceptación, entonces se implica el fallo o respuesta negativa.\n\n\n\n\n\n\nNota\n\n\n\nAlgunas causas de fallo son que la función de transición no este definida para un estado y símbolo particular y el no alcanzar un estado de salida aceptación.\n\n\nPor ejemplo, sea \\(M_{01}\\) una máquina capaz de detectar \\(n \\geq 0\\) ceros terminados por un único \\(1\\). Entonces\n\\[M_{01} = (\\{q_0, q_1, q_2\\}, \\{\\mathtt{0}, \\mathtt{1}\\}, \\{\\mathtt{0}, \\mathtt{1}, \\mathtt{X}, \\mathtt{\\epsilon} \\}, q_0, \\mathtt{\\epsilon}, \\{q_2\\}, \\delta_{01})\\]\ndonde la función de transición se define como:\n\\[\\begin{align}\n\\delta_{01}(q_0, \\mathtt{0}) &= (q_0, \\mathtt{X}, \\mathtt{R}) \\\\\n\\delta_{01}(q_0, \\mathtt{1}) &= (q_1, \\mathtt{X}, \\mathtt{R}) \\\\\n\\delta_{01}(q_1, \\mathtt{\\epsilon}) &= (q_2, \\mathtt{\\epsilon}, \\mathtt{R}) \\\\\n\\end{align}\\]\nDada la regularidad de la función de transición, es común escribirla como una tabla:\n\n\n\nentrada\nsalida\n\n\n\n\n\\((q_0, \\mathtt{0})\\)\n\\((q_0, \\mathtt{X}, \\mathtt{R})\\)\n\n\n\\((q_0, \\mathtt{1})\\)\n\\((q_1, \\mathtt{X}, \\mathtt{R})\\)\n\n\n\\((q_1, \\mathtt{\\epsilon})\\)\n\\((q_2, \\mathtt{\\epsilon}, \\mathtt{R})\\)\n\n\n\nUna máquina de Turing se puede representar mediante un autómata\n\n\n\n\n\n\n\n\nG\n\n\n\nq0\n\n\nq0\n\n\n\nq0-&gt;q0\n\n\n0 → X, R\n\n\n\nq1\n\nq1\n\n\n\nq0-&gt;q1\n\n\n1 → X, R\n\n\n\nq2\n\n\nq2\n\n\n\nq1-&gt;q2\n\n\nϵ → ϵ, R\n\n\n\n\n\n\nFigura 2.2: Ejemplo de máquina de Turing que reconoce cadenas \\(0^n 1\\)\n\n\n\n\n\nSupongamos ahora el problema de reconocer cadenas \\(0^n 1^n\\); para esto podemos definir la siguiente máquina de Turing \\[M = (\\{q_0, q_1, q_2, q_3, q_4\\}, \\{\\mathtt{0}, \\mathtt{1}\\}, \\{\\mathtt{0}, \\mathtt{1}, \\mathtt{X}, \\mathtt{Y}, \\mathtt{\\epsilon}\\}, q_0, \\mathtt{\\epsilon}, \\{q_4\\}, \\delta),\\] donde la función de transición es como sigue:\n\\[\\begin{align*}\n\\delta(q_0, \\mathtt{0}) &= (q_1, \\mathtt{X}, \\mathtt{R}) \\\\\n\\delta(q_0, \\mathtt{Y}) &= (q_3, \\mathtt{Y}, \\mathtt{R}) \\\\\n\\delta(q_1, \\mathtt{0}) &= (q_1, \\mathtt{0}, \\mathtt{R}) \\\\\n\\delta(q_1, \\mathtt{1}) &= (q_2, \\mathtt{Y}, \\mathtt{L}) \\\\\n\\delta(q_1, \\mathtt{Y}) &= (q_1, \\mathtt{Y}, \\mathtt{R}) \\\\\n\\delta(q_2, \\mathtt{0}) &= (q_2, \\mathtt{0}, \\mathtt{L}) \\\\\n\\delta(q_2, \\mathtt{X}) &= (q_0, \\mathtt{X}, \\mathtt{R}) \\\\\n\\delta(q_2, \\mathtt{Y}) &= (q_2, \\mathtt{Y}, \\mathtt{L}) \\\\\n\\delta(q_3, \\mathtt{Y}) &= (q_3, \\mathtt{Y}, \\mathtt{R}) \\\\\n\\delta(q_3, \\mathtt{\\epsilon}) &= (q_4, \\mathtt{\\epsilon}, \\mathtt{R}) \\\\\n\\end{align*}\\]\n\n\n\n\n\n\n\n\nG\n\n\n\nq0\n\n\nq0\n\n\n\nq1\n\nq1\n\n\n\nq0-&gt;q1\n\n\n0 → X, R\n\n\n\nq3\n\nq3\n\n\n\nq0-&gt;q3\n\n\nY → Y, R\n\n\n\nq1-&gt;q1\n\n\n0 → 0, R\n\n\n\nq1-&gt;q1\n\n\nY → Y, R\n\n\n\nq2\n\nq2\n\n\n\nq1-&gt;q2\n\n\n1 → Y, L\n\n\n\nq2-&gt;q0\n\n\nX → X, R\n\n\n\nq2-&gt;q2\n\n\nY → Y, L\n\n\n\nq2-&gt;q2\n\n\n0 → 0, L\n\n\n\nq3-&gt;q3\n\n\nY → Y, R\n\n\n\nq4\n\n\nq4\n\n\n\nq3-&gt;q4\n\n\nϵ → ϵ, R\n\n\n\n\n\n\nFigura 2.3: Ejemplo de máquina de Turing que reconoce cadenas \\(0^n 1^n\\)\n\n\n\n\n\nTodo inicia con una cadena de la forma esperada, e.g., \\(\\mathtt{000111}\\), la idea general del algoritmo es aparear \\(\\mathtt{0}\\)’s y \\(\\mathtt{1}\\)’s, ya que solo es valido si ambas subcadenas tienen igual longitud. Para esto se marcan los \\(\\mathtt{0}\\)’s vistos con \\(\\mathtt{X}\\) y los \\(\\mathtt{1}\\)’s con \\(\\mathtt{Y}\\). La máquina estará entonces marcando las primeras ocurrencias y moviendose a traves de la cinta para encontrar los correspondientes.\n\n\n\n\n\n\nImportante\n\n\n\nLas máquinas de Turing son capaces del representar cualquier problema cómputable con una analogía mecánica, lo cual hace evidente su implementación en el mundo físico.\n\n\n\n\n2.2.2 Modelos funcionales\n\nFunciones recursivas. Se basa en funciones que trabajan sobre los números naturales y que definen en conjunto el espacio de funciones computables. Son una herramienta abstracta que permite a los teóricos de la lógica y computación establecer los límites de lo computable.\nCálculo lambda. Es un módelo creado por Alonzo Church y Stephen Kleene a principios del siglo XX, al igual que las funciones recursivas, se fundamenta en el uso de funciones y es una herramienta abstracta con própositos similares, sin embargo el cálculo lambda no se limita a recursiones, y se enfoca en diferentes reglas de reducción y composición de funciones, y es natural la inclusión de operadores de alto nivel, aunque estos mismos sean definidos mediante un esquema funcional.\n\n\n\n2.2.3 Máquina de acceso aleatorio (RAM)\nEs un módelo que describe una computadora con registros. A diferencia de una computadora física, no tienen limitación en su capacidad, ni en la cantidad de registros, ni en la precisión de los mismos. Cada registro puede ser identicado de manera única y su contenido leído y escrito mediante reglas o instrucciones formando un programa. En particular reconoce las diferencias entre registros de los programas y registros de datos, i.e., arquitectura harvard. Existe un número mínimo de instrucciones necesarias (i.e., incremento, decremento, poner a cero, copiar, salto condicional, parar) pero es común construir esquemas más complejos basados en estas primitivas. Se necesita un registro especial que indica el registro de programa siendo ejecutado. Los accesos a los registros tienen un tiempo constante a diferencia de otros esquemas; es el modelo más cercano a como funciona una computadora moderna entre los que se hemos revisado.\n\nUna computadora moderna difieres de una máquina RAM, por ejemplo, a diferencia de una computadora física se suponen infinitos registros con precisión infinita. Debido a los costos de los semiconductores y la energía necesaría, es conveniente construir computadoras con una jerarquía de memoria: los niveles con mayores prestaciones (e.g., rápidez) son los más escasos. Es importante sacar provecho de esta jerarquía siempre que sea posible. Las operaciones también tienen costos diferentes, dependiendo de su implementación a nivel de circuitería, así como también existe cierto nivel de paralelización que no esta presente en una máquina RAM, tanto a nivel de procesamiento y lectura de datos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción al análisis de algoritmos con Julia</span>"
    ]
  },
  {
    "objectID": "cap2-analisis.html#sobre-la-importancia-del-modelo-de-cómputo-en-el-curso",
    "href": "cap2-analisis.html#sobre-la-importancia-del-modelo-de-cómputo-en-el-curso",
    "title": "2  Introducción al análisis de algoritmos con Julia",
    "section": "2.3 Sobre la importancia del modelo de cómputo en el curso",
    "text": "2.3 Sobre la importancia del modelo de cómputo en el curso\nEn este curso nos enfocaremos en especificaciones de alto nivel, donde los algoritmos son convenientes para una computadora física. Sin embargo, estaremos contando operaciones de interés pensando en costos constantes en el acceso a memoria y en una selección de operaciones, al estilo de una máquina RAM.\nLa selección de operaciones de interés tiene el espíritu de simplificar el análisis, focalizando nuestros esfuerzos en operaciones que acumulan mayor costo y que capturan la dinámica del resto. Adicionalmente al conteo de operaciones nos interesa el desempeño de los algoritmos en tiempo como magnitud física medible, así como en la cantidad de memoria consumida, por lo que se aboradará el costo realizando mediciones experimentales. Se contrastará con el análisis basado en conteo de operaciones siempre que sea posible.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción al análisis de algoritmos con Julia</span>"
    ]
  },
  {
    "objectID": "cap2-analisis.html#tipos-de-análisis",
    "href": "cap2-analisis.html#tipos-de-análisis",
    "title": "2  Introducción al análisis de algoritmos con Julia",
    "section": "2.4 Tipos de análisis",
    "text": "2.4 Tipos de análisis\nLa pregunta inicial sería ¿qué nos interesa saber de un algoritmo que resuelve un problema? probablemente, lo primero sería saber si produce resultados correctos. Después, entre el conjunto de las alternativas que producen resultados correctos, es determinante obtener su desempeño para conocer cuál es más conveniente para resolver un problema.\nEn ese punto, es necesario reconocer que para un problema, existen diferentes instancias posibles, esto es el espacio de instancias del problema, y que cada una de ellas exigirían soluciones con diferentes costos para cada algoritmo. Por tanto existen diferentes tipos de análisis y algoritmos.\n\nAnálisis de mejor caso. Obtener el mínimo de resolver cualquier instancia posible, puede parecer poco útil desde el punto de vista de decisión para la selección de un algoritmo, pero puede ser muy útil para conocer un problema o un algoritmo.\nAnálisis de peor caso. Obtener el costo máximo necesario para resolver cualquier instancia posible del problema con un algoritmo, este es un costo que si nos puede apoyar en la decisión de selección de un algoritmo; sin embargo, en muchas ocasiones, puede ser poco informativo o innecesario ya que tal vez hay pocas instancias que realmente lo amériten.\nAnálisis promedio. Se enfoca en obtener un análisis promedio basado en la población de instancias del problema para un algoritmo dado.\nAnálisis amortizado. Se enfoca en análisis promedio pero para una secuencia de instancias.\nAnálisis adaptativo. Para un subconjunto bien caracterizado del espacio de instancias de un problema busca análizar los costos del algoritmo en cuestión. La caracterización suele estar en términos de una medida de complejidad para el problema; y la idea general es medir si un algoritmo es capaz de sacar provecho de instancias fáciles.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción al análisis de algoritmos con Julia</span>"
    ]
  },
  {
    "objectID": "cap2-analisis.html#notación-asintótica",
    "href": "cap2-analisis.html#notación-asintótica",
    "title": "2  Introducción al análisis de algoritmos con Julia",
    "section": "2.5 Notación asintótica",
    "text": "2.5 Notación asintótica\nRealizar un conteo de operaciones y mediciones es un asunto complejo que requiere focalizar los esfuerzos. Para este fin, es posible contabilizar solo algunas operaciones de importancia, que se supondrían serían las más costosas o que de alguna manera capturan de manera más fiel la dinámica de costos.\nEl comportamiento asintótico es otra forma de simplificar y enfocarnos en los puntos de importancia, en este caso, cuando el tamaño de la entrada es realmente grande. Es importante mencionar, que no se esperan entradas de tamaño descomunal, ni tampoco se espera cualquier tipo de entrada.\n\n2.5.1 Notación \\(\\Theta\\)\nPara una función dada \\(g(n)\\) denotamos por \\(\\Theta(g(n))\\) el siguiente conjunto de funciones:\n\\[\\begin{align}\n\\Theta(g(n)) &=  \\left\\{ f(n) \\mid \\text{ existen las constantes positivas }c_1, c_2 \\text{ y } n_0 \\text{ tal que } \\right.\\\\\n    ~ & \\left. 0 \\leq c_1 g(n) \\leq f(n) \\leq c_2 g(n) \\text{ para todo } n \\geq n_0 \\right\\} \\\\\n\\end{align}\\]\nesto es, una función \\(f(n)\\) pertenece al conjunto \\(g(n)\\) si \\(c_1 g(n)\\) y \\(c_2 g(n)\\) pueden cubrirla por abajo y por arriba, para esto deben existen las constantes positivas \\(c_1\\) y \\(c_2\\) y una \\(n\\) lo suficientemente larga, e.g., para eso la constante \\(n_0\\). La notación propiamente de conjuntos puede usarse \\(f(n) \\in \\Theta(g(n))\\) pero es común en el área usar \\(f(n) = \\Theta(g(n))\\) para expresar la pertenencia; este abuso de la notación tiene ventaja a la hora de plantear los problemas de análisis.\n\n\n2.5.2 Notación \\(O\\)\nSe utiliza para indicar una cota asintótica superior. Una función \\(f(n)\\) se dice que esta en \\(O(g(n))\\) si esta en el siguiente conjunto:\n\\[\\begin{align}\nO(g(n)) &=  \\left\\{ f(n)  \\mid \\text{ existen las constantes positivas }c \\text{ y } n_0 \\text{ tal que } \\right.\\\\\n    ~& \\left. 0 \\leq f(n) \\leq c g(n) \\text{ para todo } n \\geq n_0 \\right\\} \\\\\n\\end{align}\\]\nLa notación \\(O\\) se usa para dar una cota superior, dentro de un factor constante. Al escribir \\(f(n) = O(g(n))\\) se indica que \\(f(n)\\) es miembro del conjunto \\(O(g(n))\\); hay que notar que \\(f(n) = \\Theta(g(n))\\) implica que \\(f(n) = O(g(n))\\), i.e., \\(\\Theta(g(n)) \\subseteq O(g(n))\\).\n\n\n2.5.3 Notación \\(\\Omega\\)\nAl contrario de \\(O\\), la notación \\(\\Omega\\) da una cota asintótica inferior. Una función \\(f(n)\\) se dice que esta en \\(\\Omega(g(n))\\) si esta en el siguiente conjunto:\n\\[\\begin{align}\n\\Omega(g(n)) = & \\left\\{ f(n)  \\mid \\text{ existen las constantes positivas }c \\text{ y } n_0 \\text{ tal que } \\right. \\\\\n    & \\left. 0 \\leq c g(n) \\leq f(n) \\text{ para todo } n \\geq n_0 \\right\\} \\\\\n\\end{align}\\]\nDado que la \\(\\Omega\\) define una cota superior, basicamente si \\(f(n) = \\Omega(g(n))\\), entonces \\(f(n)\\) debe estar por encima de \\(g(n)\\) con las constantes \\(c\\) y \\(n_0\\) adecuadas. Al igual que la notación \\(O\\), la notación \\(\\Omega\\) es menos estricta que \\(\\Theta\\), esto es \\(f(n) = \\Theta(g(n))\\) implica que \\(f(n) = \\Omega(g(n))\\), por lo que \\(\\Theta(g(n)) \\subseteq \\Omega(g(n))\\).\nPor tanto, si \\(f(n) = O(g(n))\\) y \\(f(n) = \\Omega(g(n))\\) entonces \\(f(n) \\in \\Theta(g(n))\\).\n\nEs importante conocer los ordenes de crecimiento más comunes de tal forma que podamos realizar comparaciones rápidas de costos, y dimensionar las diferencias de recursos entre diferentes tipos de costos. La notación asintótica hace uso extensivo de la diferencia entre diferentes ordenes de crecimiento para ignorar detalles y simplificar el análisis de algoritmos.\n\n\n\n2.5.4 Apoyo audio-visual\nEn los siguientes videos se profundiza sobre los modelos de cómputo y los diferentes tipos de análisis sobre algoritmos.\n\nParte 1: \nParte 2: \nParte 3: \n\n\n\n2.5.5 Ordenes de crecimiento\n\n\nDado que la idea es realizar un análisis asintótico, las constantes suelen ignorarse, ya que cuando el tamaño de la entrada es suficientemente grande, los términos con mayor orden de magnitud o crecimiento dominarán el costo. Esto es, es una simplificación necesaría.\nLos ordenes de crecimiento son maneras de categorizar la velocidad de crecimiento de una función, y para nuestro caso, de una función de costo. Junto con la notación asimptótica nos permite concentrarnos en razgos gruesos que se mantienen para entradas grandes, más que en los detalles, y no perder el punto de interés. A continuación veremos algunas funciones con crecimientos paradigmáticos; las observaremos de poco en poco para luego verlos en conjunto.\n\n2.5.5.1 Costo constante, logaritmo y lineal\nLa siguiente figura muestra un crecimiento nulo (constante), logaritmico y lineal. Note como la función logarítmica crece lentamente.\n\nusing Plots, LaTeXStrings\nn = 300 # 300 puntos\n\nplot(1:n, [10 for x in 1:n], label=L\"c\")\nplot!(1:n, [log2(x) for x in 1:n], label=L\"\\log{n}\")\nplot!(1:n, [x for x in 1:n], label=L\"n\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.5.2 Costo \\(n \\log n\\) y polinomial\nA continuación veremos tres funciones, una función con \\(n\\log n\\) y una función cuadrática y una cúbica. Note como para valores pequeños de \\(n\\) las diferencias no son tan apreciables para como cuando comienza a crecer \\(n\\); así mismo, observe los valores de \\(n\\) de las figuras previas y de la siguiente, este ajuste de rangos se hizo para que las diferencias sean apreciables.\n\nn = 7 # note que se usan menos puntos porque 300 serían demasiados para el rango\n\nplot(1:n, [x * log2(x) for x in 1:n], label=L\"n\\log_2{n}\")\nplot!(1:n, [x^2 for x in 1:n], label=L\"n^2\")\nplot!(1:n, [x^3 for x in 1:n], label=L\"n^3\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.5.3 Exponencial\nA continuación se compara el crecimiento de una función exponencial con una función polinomial. Note que la función polinomial es de grado 4 y que la función exponencial tiene como base 2; aún cuando para números menores de aproximadamente 16 la función polinomial es mayor, a partir de ese valor la función \\(2^n\\) supera rapidamente a la polinomial.\n\nn = 20\n\nplot(1:n, [x^4 for x in 1:n], label=L\"n^4\")\nplot!(1:n, [2^x for x in 1:n], label=L\"2^n\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.5.4 Crecimiento factorial\nVease como la función factorial crece mucho más rápido que la función exponencial para una \\(n\\) relativamente pequeña. Vea las magnitudes que se alcanzan en el eje \\(y\\), y comparelas con aquellas con los anteriores crecimientos.\n\nn = 20\n\nplot(1:n, [2^x for x in 1:n], label=L\"2^n\")\nplot!(1:n, [factorial(x) for x in 1:n], label=L\"n!\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.5.5 Un poco más sobre funciones de muy alto costo\n\nn = 10\n\nplot(1:n, [factorial(x) for x in 1:n], label=L\"n!\")\nplot!(1:n, [x^x for x in Int128(1):Int128(n)], label=L\"n^n\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVea la figura anterior, donde se compara \\(n!\\) con \\(n^n\\), observe como es que cualquier constante se vuelve irrelevante rapidamente; aun para \\(n^n\\) piense en \\(n^{n^n}\\).\nNote que hay problemas que son realmente costosos de resolver y que es necesario conocer si se comporta así siempre, si es bajo determinado tipo de entradas. Hay problemas en las diferentes áreas de la ciencia de datos, donde veremos este tipo de costos, y habrá que saber cuando es posible solucionarlos, o cuando se deben obtener aproximaciones que nos acerquen a las respuestas correctas con un costo manejable, es decir, mediar entre exactitud y costo. En este curso se abordaran problemas con un costo menor, pero que por la cantidad de datos, i.e., \\(n\\), se vuelven muy costosos y veremos como aprovechar supuestos como las distribuciones naturales de los datos para mejorar los costos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción al análisis de algoritmos con Julia</span>"
    ]
  },
  {
    "objectID": "cap2-analisis.html#el-enfoque-experimental",
    "href": "cap2-analisis.html#el-enfoque-experimental",
    "title": "2  Introducción al análisis de algoritmos con Julia",
    "section": "2.6 El enfoque experimental",
    "text": "2.6 El enfoque experimental\nLa notación asintótica nos permite alcanzar un lenguaje común y preciso sobre los costos de problemas y algoritmos; es de especial importancia para la evaluación de las alternativas en la literatura especializada, y elegir algoritmos aún sin la necesidad de implementación. El análisis asintótico da la posibilidad de conocer el desempeño desde diferentes perspectivas como peor caso o caso promedio, utilizando un módelo de computación, y siempre pensando en entradas lo suficientemente grandes.\nEn la práctica, existe una múltitud de razones por los cuales los problemas que se resuelven podrian no ser tan grandes como para que un algoritmo domine a otros de manera asintótica, las instancias podrían no ser tan generales como para preocuparse en el peor caso, o el caso promedio general. En muchas situaciones, es importante sacar provecho de los casos fáciles, sobre todo cuando el problema a resolver podría asegurar que dichos casos simples sean abundantes. Dada la complejidad detrás de definir sub-conjuntos de instancias y llevar a cabo un análisis formal, se vuelve imperativo realizar pruebas experimentales.\nPor otra parte, dada la complejidad de una computadora moderna, es necesario realizar evaluaciones experimentales de los algoritmos que tengan una complejidad similar. Las computadoras reales tienen una jerarquia de memoria con tamaños y velocidades de acceso divergentes entre sí, con optimizaciones integradas sobre la predicción de acceso y cierto nivel de paralelismo. Incluso, cada cierto tiempo se obtienen optimizaciones en los dispositivos que podrían mejorar los rendimientos, por lo que es posible que con una generación a otra, lo que sabemos de los algoritmos y su desempeño en computadoras y cargas de trabajo reales cambie.\n\n2.6.1 Metodología experimental\nAlgunos de los algoritmos que se verán en este libro son sumamente rapidos en la práctica para resolver una instancia práctica por lo que medir el desempeño de instancias solas podría no tener sentido. La acumulación de operaciones es fundamental, así como la diversidad de las instancias también lo es. Caracterizar las entradas es de vital importancia ya que la adaptabilidad a las instancias es parte de los objetivos.\nEntonces, estaremos probando conjuntos de instancias, caracterizadas y estaremos utilizando tiempos promedios. También estaremos usando conteo de operaciones, por lo que los algoritmos en cuestión muchas veces serán adaptados para poder realizar este conteo.\nEn Julia etaremos utilizando las siguientes instrucciones:\n\n@time expr macro que mide el tiempo en segundo utilizado por expr, también reporta el número de alojaciones de memoria. Note que reducir la cantidad de memoria alojada puede significar reducir el tiempo de una implementación, ya que el manejo de memoria dinámica es costoso.\n@benchmark expr params macro del paquete BenchmarkTools que automatiza la repetición de expr para obtener diferentes mediciones y hacer un reporte, params permite manipular la forma en que se reliza la evaluación.\n@btime expr params macro del paquete BenchmarkTools que mimetiza la salida de @time.\n\n\na = rand(Float32, 3, 3)\n1@time a * a\n2@time a * a\n\n\n1\n\nTodas las fuciones se deben compilar, la primera llamada uncluye los costos de compilación.\n\n2\n\nEl costo sin compilación, hay una alojación que es la matriz donde se guarda el resultado.\n\n\n\n\n  0.841567 seconds (2.01 M allocations: 136.608 MiB, 3.33% gc time, 99.97% compilation time)\n  0.000005 seconds (1 allocation: 96 bytes)\n\n\n3×3 Matrix{Float32}:\n 0.635295  0.150821  0.310147\n 1.02056   0.23071   0.674775\n 1.71173   0.369326  1.20393\n\n\nTanto @benchmark como @btime aceptan interpolación de variables con el prefijo $ para controlar la evaluación de una expresión se debe contar como parte de lo que se quiere medir o no. Se puede combinar con el parametro setup para controlar de manera precisa las entradas para evaluar cada una de las repeticiones de expr.\n\nusing BenchmarkTools\n\n@benchmark a * a setup=(a=rand(Float32, 3, 3))\n\nBenchmarkTools.Trial: 10000 samples with 985 evaluations.\n Range (min … max):  46.417 ns …  2.232 μs  ┊ GC (min … max): 0.00% … 96.36%\n Time  (median):     48.330 ns              ┊ GC (median):    0.00%\n Time  (mean ± σ):   51.039 ns ± 35.694 ns  ┊ GC (mean ± σ):  2.23% ±  3.83%\n\n  ▁▃▂▃██▄▃▅▄▃▃▂▂▂▂▃▃▃▃▃▃▂▁▁▁▁▁                                ▂\n  ████████████████████████████████▇▇▇▆▇▆▆▇▆▅▆▇▆▆▅▅▅▄▅▅▁▄▄▄▅▄▅ █\n  46.4 ns      Histogram: log(frequency) by time      67.6 ns &lt;\n\n Memory estimate: 96 bytes, allocs estimate: 1.\n\n\n\n\na = rand(Float32, 3, 3)\n@btime a * a setup=(a=$a)\n\n  47.875 ns (1 allocation: 96 bytes)\n\n\n3×3 Matrix{Float32}:\n 0.575313  0.847652  0.395764\n 0.417129  0.719546  0.322025\n 0.375838  0.367533  0.22682\n\n\nEl parametro sample controla el número máximo de muestras que se tomarán para el análisis, y seconds limita el tiempo sobre el cual se tomarán muestras; se asegura que al menos se tomará una muestra, se debe tener en cuenta que puede costar más que seconds.1\n\na = rand(Float32, 3, 3)\nb = rand(Float32, 3, 3)\n@benchmark a * b setup=(a=$a, b=$b) samples=1000 seconds=0.33\n\nBenchmarkTools.Trial: 1000 samples with 980 evaluations.\n Range (min … max):  63.631 ns … 880.123 ns  ┊ GC (min … max): 0.00% … 89.30%\n Time  (median):     70.518 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   77.375 ns ±  38.616 ns  ┊ GC (mean ± σ):  1.79% ±  3.96%\n\n  █▆▆▆▅▄▂▂▂▃▂                                                   \n  ████████████▅▄▆▄▅▁▅▅▅▅▅▄▅▅▁▄▄▅▅▁▁▄▁▁▁▁▁▁▁▅▁▁▁▁▄▄▄▄▁▅▁▄▁▁▄▄▄▄ █\n  63.6 ns       Histogram: log(frequency) by time       209 ns &lt;\n\n Memory estimate: 96 bytes, allocs estimate: 1.\n\n\n\n\n2.6.2 Ejemplo del cálculo de máximo de un arreglo y diferentes tipos de costo.\n–\n\n\n1function maximo(col)\n    maxpos = 1\n    actualizaciones = 1\n    i = 2\n\n    while i &lt; length(col)\n        if col[maxpos] &lt; col[i]\n            maxpos = i\n            actualizaciones += 1\n        end\n        i += 1\n    end\n\n    maxpos, actualizaciones\nend\n\n\n1\n\nFunción que encuentra el máximo en una secuencia y devuelve su posición, y además devuelve el número de veces que se actualizó el máximo en el recorrido.\n\n\n\n\nmaximo (generic function with 1 method)\n\n\n\na = rand(UInt32, 128)\n1@benchmark maximo($a) samples=100 seconds=3\n\n\n1\n\nUn análisis de desempeño usando @benchmark; probando con máximo 100 samples en 3 segundos.\n\n\n\n\nBenchmarkTools.Trial: 100 samples with 875 evaluations.\n Range (min … max):  154.687 ns … 215.714 ns  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     160.295 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   162.588 ns ±   8.815 ns  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n  █        ▂         ▁                                           \n  █▆▃▃▃▅▁▁▃██▃▃▄▃▃▄▃▁█▄▁▁▁▁▃▁▁▁▃█▃▃▁▁▁▁▁▁▁▁▃▃▁▁▁▁▃▁▁▁▁▁▁▁▁▁▃▁▁▃ ▃\n  155 ns           Histogram: frequency by time          188 ns &lt;\n\n Memory estimate: 0 bytes, allocs estimate: 0.\n\n\nNote que aunque se tiene un análisis muy detallado del desempeño, otras medidas de costo caen fuera del diseño del paquete, por lo que es necesario hacerlas por otros medios. Por ejemplo, suponga que el número de actualizaciones es nuesta medida de desempeño, un código donde se capturen las actualizaciones\n\n1using StatsBase\n2a = [maximo(rand(UInt32, 128))[2] for i in 1:100]\n3quantile(a, [0.0, 0.25, 0.5, 0.75, 1.0])\n\n\n1\n\nInclusión de un paquete para cálculo de estadísticas básicas.\n\n2\n\nDefinición de 100 experimentos que calculan maximo sobre arreglos aleatorios.\n\n3\n\nCálculo del mínimo, cuantiles 0.25, 0.5, 0.75, y el máximo, para determinar el desempeño.\n\n\n\n\n5-element Vector{Float64}:\n 1.0\n 4.0\n 5.0\n 6.0\n 9.0",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción al análisis de algoritmos con Julia</span>"
    ]
  },
  {
    "objectID": "cap2-analisis.html#actividades",
    "href": "cap2-analisis.html#actividades",
    "title": "2  Introducción al análisis de algoritmos con Julia",
    "section": "2.7 Actividades",
    "text": "2.7 Actividades\nComparar mediante simulación en un notebook de Jupyter o Quarto los siguientes órdenes de crecimiento:\n\n\\(O(1)\\) vs \\(O(\\log n)\\)\n\\(O(n)\\) vs \\(O(n \\log n)\\)\n\\(O(n^2)\\) vs \\(O(n^3)\\)\n\\(O(a^n)\\) vs \\(O(n!)\\)\n\\(O(n!)\\) vs \\(O(n^n)\\)\nEscoja los rangos adecuados para cada comparación, ya que como será evidente después, no es práctico fijar los rangos.\nCree una figura por comparación, i.e., cinco figuras. Discuta lo observado por figura.\nCree una tabla donde muestre tiempos de ejecución simulados para algoritmos ficticios que tengan los órdenes de crecimiento anteriores, suponiendo que cada operación tiene un costo de 1 nanosegundo.\n\nUse diferentes tamaños de entrada \\(n=100\\), \\(n=1000\\), \\(n=10000\\) y \\(n=100000\\).\nNote que para algunas fórmulas, los números pueden ser muy grandes, tome decisiones en estos casos y defiendalas en el reporte.\n\nDiscuta las implicaciones de costos de cómputo necesarios para manipular grandes volúmenes de información, en el mismo notebook.\n\n\n2.7.1 Entregable\nSu trabajo se entregará en PDF y con el notebook fuente; deberá estar plenamente documentado, con una estructura que permita a un lector interesado entender el problema, sus experimentos y metodología, así como sus conclusiones. Tenga en cuenta que los notebooks pueden alternar celdas de texto y código.\nNo olvide estructurar su reporte, en particular el reporte debe cubrir los siguientes puntos:\n\nTítulo del reporte, su nombre.\nIntroducción.\nCódigo cercano a la presentación de resultados.\nFiguras y comparación de los órdenes de crecimiento.\nAnálisis y simulación de costo en formato de tabla.\nConclusión. Debe abordar las comparaciones hechas y la simulación; también toque el tema de casos extremos y una \\(n\\) variable y asintóticamente muy grande.\nLista de referencias. Nota, una lista de referencias que no fueron utilizadas en el cuerpo del texto será interpretada como una lista vacía.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción al análisis de algoritmos con Julia</span>"
    ]
  },
  {
    "objectID": "cap2-analisis.html#bibliografía",
    "href": "cap2-analisis.html#bibliografía",
    "title": "2  Introducción al análisis de algoritmos con Julia",
    "section": "2.8 Bibliografía",
    "text": "2.8 Bibliografía\nCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2022). Introduction to Algorithms (2nd ed.). MIT Press.\n\nParte I: Cap. 1, 2, 3",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción al análisis de algoritmos con Julia</span>"
    ]
  },
  {
    "objectID": "cap2-analisis.html#footnotes",
    "href": "cap2-analisis.html#footnotes",
    "title": "2  Introducción al análisis de algoritmos con Julia",
    "section": "",
    "text": "Se recomienda visitar el sitio https://juliaci.github.io/BenchmarkTools.jl/stable/ para más información sobre el paquete BenchmarkTools, y en particular para sus parametros, como guardar información de corridas.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción al análisis de algoritmos con Julia</span>"
    ]
  },
  {
    "objectID": "cap3-estructuras.html",
    "href": "cap3-estructuras.html",
    "title": "3  Estructuras de datos elementales",
    "section": "",
    "text": "Objetivo\nImplementar, aplicar y caracterizar el desempeño de algoritmos en peor caso y adaptativos para búsqueda en arreglos ordenados. Se discutirán estructuras de datos básicas que serán de gran utilidad al momento de construir programas y de resolver problemas más complejos; nos enfocaremos en las estructuras de datos.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estructuras de datos elementales</span>"
    ]
  },
  {
    "objectID": "cap3-estructuras.html#introducción",
    "href": "cap3-estructuras.html#introducción",
    "title": "3  Estructuras de datos elementales",
    "section": "3.1 Introducción",
    "text": "3.1 Introducción\nEn esta unidad se discutirán las propiedades y operaciones básicas de estructuras como conjuntos, listas, pilas, colas, arreglos, vectores, matrices y matrices dispersas. Los ejemplos de código se muestran en el lenguaje de programación Julia, pero que puede ser traducido fácilmente en otros lenguajes de programación.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estructuras de datos elementales</span>"
    ]
  },
  {
    "objectID": "cap3-estructuras.html#conjuntos",
    "href": "cap3-estructuras.html#conjuntos",
    "title": "3  Estructuras de datos elementales",
    "section": "3.2 Conjuntos",
    "text": "3.2 Conjuntos\nLos conjuntos son estructuras abstractas que representan una colección de elementos, en particular, dado las posibles aplicaciones un conjunto puede tener contenido inmutable o mutable. Un conjunto puede estar vacio (\\(\\emptyset\\)) o contener elementos, e.g., \\(\\{a, b, c\\}\\). La operación unión \\(\\cup\\) construye un nuevo conjunto a partir de otros \\(\\{a, b\\} \\cup \\{c\\} = \\{a, b, c\\}\\); la intersección se indica con el operador \\(\\cap\\), e.g. \\(\\{a, b, c\\} \\cap \\{b, d\\} = \\{b\\}\\). El tamaño de una colección lo representamos con barras, e.g., \\(|\\{a, b\\}| = 2\\). También es útil consultar por membresia \\(a \\in \\{a, b, c\\}\\) o por su negación, i.e., \\(d \\not\\in \\{a, b, c\\}\\). Es común usar conjuntos mutables en diferentes algoritmos, esto es, que permitan inserciones y borrados sobre la misma estructura; desde el punto de vista de eficiencia, esto puede reducir las operaciones de manipulación de datos así como de la gestión de memoria. Suponga el conjunto \\(S = \\{a, b, c\\}\\), la función \\(pop!(S, b)\\) resultaría en \\(\\{a, c\\}\\), y la función \\(push!(S, d)\\) resultaría en \\(\\{a, c, d\\}\\) al encadenar estas operaciones. Note que el símbolo \\(!\\) solo se esta usando en cooncordancia con el lenguaje de programación Julia para indicar que la función cambiaría el argumento de entrada; es una convención, no un operador en sí mismo. Note que estamos usando una sintaxis muy sencilla \\(fun(arg1, arg2, ...)\\) para indicar la aplicación de una función u operación a una serie de argumentos.\nHay múltiples formas de representar conjuntos ya que los requerimientos de los algoritmos son diversos y tener la representación correcta puede ser una diferencia importante en el rendimiento. Las implementaciones y algoritmos alrededor pueden llegar a ser muy sofisticados, dependiendo de las características que se desean, algunas de las cuales serán el centro de estudio de este curso.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estructuras de datos elementales</span>"
    ]
  },
  {
    "objectID": "cap3-estructuras.html#tuplas-y-estructuras",
    "href": "cap3-estructuras.html#tuplas-y-estructuras",
    "title": "3  Estructuras de datos elementales",
    "section": "3.3 Tuplas y estructuras",
    "text": "3.3 Tuplas y estructuras\nLas tuplas son colecciones abstractas ordenadas, donde incluso puede haber repetición, pueden verse como una secuencia de elementos, e.g., \\(S = (a, b, c)\\); podemos referirnos a la \\(i\\)ésima posición de la forma \\(S_i\\), o incluso \\(S[i]\\), si el contexto lo amerita, e.g., pseudo-código que pueda ser transferido a un lenguaje de programación más fácilmente. Es común que cada parte de la tupla pueda contener cierto tipo de dato, e.g., enteros, números de punto flotante, símbolos, cadenas de carácteres, etc. Una tupla es muy amena para ser representada de manera contigua en memoria. En el lenguaje de programación Julia, las tuplas se representan entre paréntesis, e.g., \\((1, 2, 3)\\).\n\nt = (10, 20, 30)\n\nt[1] * t[3] - t[2]\n\n280\nDefinición y acceso a los campos de una tupla en Julia\n\n\n\n\nEn algunos lenguajes de programación como Julia, una tupla puede enviarse como valor (copiar) cuando se utiliza en una función; por lo mismo, puede guardarse en el stack, que es la memoria inmediata que se tiene en el contexto de ejecución de una función. En esos casos, se puede optimizar el manejo de memoria (alojar y liberar), lo cuál puede ser muy beneficioso para un algoritmo en la práctico. El otro esquema posible es el heap, que es una zona de memoria que debe gestionarse (memoria dinámica); es más flexible y duradera entre diferentes llamadas de funciones en un programa. Los patrones esperados son dispersos y puede generar fragmentación.\nUna estructura es una tupla con campos nombrados; es muy útilizada en lenguajes de programación, por ejemplo, en Julia la siguiente estructura puede representar un punto en un plano:\n\nstruct Point\n  x::Float32\n  y::Float32\nend\n\nNote la especificación de los tipos de datos que en conjunto describirán como dicha estructura se maneja por una computadora, y que en términos prácticos, es determinante para el desempeño. Es común asignar valores satelitales en programas o algoritmos, de tal forma que un elemento simple sea manipulado o utilizado de manera explicita en los algoritmos y tener asociados elementos secundarios que se vean afectados por las operaciones. Los conjuntos, tuplas y las estructuras son excelentes formas de representar datos complejos de una manera sencilla.\nEn Julia, es posible definir funciones o métodos al rededor del tipo de tuplas y estructuras.\n\n\nEs importante saber que si algunos de los campos o datos de una tupla o estructura estan en el heap entonces solo una parte estará en el stack; i.e., en el caso extremo solo serán referencias a datos en el heap. Esto puede llegar a complicar el manejo de memoria, pero también puede ser un comportamiento sobre el que se puede razonar y construir.\n\n\"\"\"\n  Calcula la norma de un vector representado\n  como un tupla\n\"\"\"\nfunction norm(u::Tuple)\n  s = 0f0\n  for i in eachindex(u)\n    s = u[i]^2\n  end\n  sqrt(s)\nend\n\n\"\"\"\n  Calcula la norma de un vector de 2 dimensiones\n  representado como una estructura\n\"\"\"\nfunction norm(u::Point)\n  sqrt(u.x^2 + u.y^2)\nend\n\n(norm((1, 1, 1, 1)), norm(Point(1, 1)))\n\n(1.0, 1.4142135f0)\nFunciones sobre diferentes tipos de datos\n\n\nNote que la función es diferente para cada tipo de entrada; a este comportamiento se le llamada despacho múltiple y será un concepto común este curso. En otros lenguajes de programación se implementa mediante orientación a objetos.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estructuras de datos elementales</span>"
    ]
  },
  {
    "objectID": "cap3-estructuras.html#arreglos",
    "href": "cap3-estructuras.html#arreglos",
    "title": "3  Estructuras de datos elementales",
    "section": "3.4 Arreglos",
    "text": "3.4 Arreglos\nLos arreglos son estructuras de datos que mantienen información de un solo tipo, tienen un costo constante \\(O(1)\\) para acceder a cualquier elemento (también llamado acceso aleatorio) y tipicamente se implementan como memoria contigua en una computadora. Al igual que las tuplas, son colecciones ordenadas, las estaremos accediendo a sus elementos con la misma notación. En este curso usaremos arreglos como colecciones representadas en segmentos contiguos de memoria con dimensiones lógicas fijas. A diferencia de las tuplas, es posible reemplazar valores, entonces \\(S_{ij} \\leftarrow a\\), reemplazará el contenido de \\(S\\) en la celda especificada por \\(a\\).\n\n\nJulia tiene un soporte para arreglos excepcional, el cual apenas trataremos ya que se enfoca en diferentes áreas del cómputo numérico, y nuestro curso esta orientado a algoritmos. En Python, estructuras similares se encuentra en el paquete Numeric Python o numpy; tenga en cuenta que las afirmaciones sobre el manejo de memoria y representación que estaremos usando se apegan a estos modelos, y no a las listas nativas de Python.\nA diferencia de las tuplas, pueden tener más que una dimensión. La notación para acceder a los elementos se extiende, e.g. para una matriz \\(S\\) (arreglo bidimensional) \\(S_{ij}\\) se refiere a la celda en la fija \\(i\\) columna \\(j\\), lo mismo que \\(S[i, j]\\). Si pensamos en datos numéricos, un arreglo unidimensional es útil para modelar un vector de múltiples dimensiones, un arreglo bidimensional para representar una mátriz de tamaño \\(m \\times n\\), y arreglos de dimensión mayor pueden usarse para tensores. Se representan en memoria en segmentos contiguos, y los arreglos de múltiples dimensiones serán representados cuyas partes pueden ser delimitadas mediante aritmética simple, e.g., una matriz de tamaño \\(m \\times n\\) necesitará una zona de memoria de \\(m \\times n\\) elementos, y se puede acceder a la primera columna mediante en la zona \\(1,\\dots,m\\), la segunda columna en \\(m+1,\\dots,2m\\), y la \\(i\\)ésima en \\((i-1)m+1,\\dots,im\\); esto es, se implementa como el acceso en lotes de tamaño fijo en un gran arreglo unidimensional que es la memoria.\n\n\nEsta es la manera que en general se manejan los datos en una computadora, y conocerlo de manera explícita nos permite tomar decisiones de diseño e implementación.\n\n\n\n\n\n\n\n\nlista\n\n\n\nRAM\n\nmemoria RAM\n\notros\ndatos\n\ncolumna 1 - x[:, 1]\n\nx[1,1]\n\nx[2,1]\n\nx[3,1]\n\nx[4,1]\n\ncolumna 2 - x[:, 2]\n\nx[1,2]\n\nx[2,2]\n\nx[3,2]\n\nx[4,2]\n\ncolumna 3 - x[:, 3]\n\nx[1,3]\n\nx[2,3]\n\nx[3,3]\n\nx[4,3]\n\ncolumna 4 - x[:, 4]\n\nx[1,4]\n\nx[2,4]\n\nx[3,4]\n\nx[4,4]\n\notros\ndatos\n\n\n\n\n\n\nFigura 3.1: Esquema de una matriz en memoria.\n\n\n\n\n\nLa representación precisa en memoria es significativa en el desempeño de operaciones matriciales como pueden ser el producto entre matrices o la inversión de las mismas. La manera como se acceden los datos es crucial en el diseño de los algoritmos.\nEl siguiente ejemplo define un vector \\(u\\) de \\(m\\) elementos y una matriz \\(X\\) de tamaño \\(m \\times n\\), ambos en un cubo unitario de 4 dimensiones, y define una función que selecciona el producto punto máximo del vector \\(u\\) a los vectores columna de \\(X\\):\n\n\nfunction mydot(u, x)\n  s = 0f0\n  for i in eachindex(u, x)\n    s += u[i] * x[i]\n  end\n  s\nend\n\nfunction getmaxdot(u::Vector, X::Matrix)\n  maxpos = 1\n  # en la siguiente linea, @view nos permite controlar que\n  # no se copien los arreglos, y en su lugar, se usen referencias\n  maxdot = mydot(u, @view X[:, 1])\n  # obtiene el número de columnas e itera apartir del 2do indice \n  mfilas, ncols = size(X)\n  for i in 2:ncols\n    d = mydot(u, @view X[:, i]) \n    if d &gt; maxdot\n      maxpos = i\n      maxdot = d\n    end\n  end\n\n  (maxpos, maxdot)\nend\n\ngetmaxdot(rand(Float32, 4), rand(Float32, 4, 1000))\n\n(917, 2.3676164f0)\n\n\nEn este código puede verse como se separa el cálculo del producto punto en una función, esto es porque en sí mismo es una operación importante; también podemos aislar de esta forma la manera que se accede (el orden) a los vectores. La idea fue acceder columna a columna, lo cuál asegura el uso apropiado de los accesos a memoria. En la función \\(getmaxdot\\) se resuelve el problema de encontrar el máximo de un arreglo, y se puede observar que sin conocimiento adicional, este requiere \\(O(n)\\) comparaciones, para una mátriz de \\(n\\) columnas. Esto implica que cada producto punto se cuenta como \\(O(1)\\), lo cual simplifica el razonamiento. Por la función \\(mydot\\) podemos observar que el producto punto tiene un costo de \\(O(m)\\), por lo que la \\(getmaxdot\\) tiene un costo de \\(O(mn)\\) operaciones lógicas y aritméticas.\nEl producto entre matrices es un caso paradigmático por su uso en la resolución de problemas prácticos, donde hay una gran cantidad de trabajo al rededor de los costos necesarios para llevarlo a cabo. En particular, el algoritmo naïve, es un algoritmo con costo cúbico, como se puede ver a continuación:\n\nfunction myprod(A::Matrix, B::Matrix)\n  mA, nA = size(A)\n  mB, nB = size(B)\n  @assert nA == mB\n  C = Matrix{Float32}(undef, mA, nB)\n\n  for i in 1:mA\n    for j in 1:mB\n      rowA = @view A[i, :]\n      colB = @view B[:, i]\n      C[i, j] = mydot(rowA, colB)\n    end\n  end\n\n  C\nend\n\nA = rand(Float32, 5, 3)\nB = rand(Float32, 3, 5)\nC = myprod(A, B)\ndisplay(C)\n\n5×5 Matrix{Float32}:\n 0.814961  0.814961  0.814961  4.2092f-41  -2.30383f32\n 0.406502  0.406502  0.406502  7.0f-45      4.2092f-41\n 0.402574  0.402574  0.402574  0.0         -2.30385f32\n 0.344123  0.344123  0.344123  1.0f-44      4.2092f-41\n 1.43786   1.43786   1.43786   0.0         -2.05116f32\nFunciones sobre diferentes tipos de datos\n\n\nSe pueden ver dos ciclos iterando a lo largo de filas y columnas, adicionalmente un producto punto, el cual tiene un costo lineal en la dimensión del vector, por lo que el costo es cúbico. Esta implementación es directa con la definición misma del producto matricial. Existen diferentes algoritmos para hacer esta operación más eficiente para diferentes casos o características de las matrices, siendo un área de investigación activa.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estructuras de datos elementales</span>"
    ]
  },
  {
    "objectID": "cap3-estructuras.html#listas",
    "href": "cap3-estructuras.html#listas",
    "title": "3  Estructuras de datos elementales",
    "section": "3.5 Listas",
    "text": "3.5 Listas\nLas listas son estructuras de datos ordenadas lineales, esto es, no se asume que los elementos se guardan de manera contigua y los accesos al \\(i\\)-ésimo elemento cuestan \\(O(i)\\). Se soportan inserciones y borrados. Por ejemplo, sea \\(L = [a, b, c, d]\\) una lista con cuatro elementos, \\(L_2 = b\\), \\(insert!(L, 2, z)\\) convertirá \\(L = [a, z, b, c, d]\\) (note que \\(b\\) se desplazó y no se reemplazó como se esperaría en un arreglo). La operación \\(deleteat!(L, 2)\\) regresará la lista a su valor previo a la inserción. Estas operaciones que modifican la lista también tienen diferentes costos dependiendo de la posición, e.g., donde el inicio y final de la secuencia (también llamados head/cabeza y tail/cola) suelen ser más eficientes que accesos aleatorios, ya que se tienen referencias a estas posiciones en memoria. Es de especial importancia la navegación por la lista mediante operaciones de sucesor \\(succ\\) y predecedor \\(pred\\), que pueden encadenarse para obtener acceso a los elementos. A diferencia de un arreglo, las listas no requieren una notación simple para acceso aleatorio a los elementos; los accesos típicos son a los extremos de la lista (cabeza y cola), sucesor y predecesor.\n\n\n\n\n\n\n\n\nlista\n\n\n\nlist\n\nhead\n\ntail\n\n\n\na\n\na\n\n \n\n\n\nlist:n-&gt;a:n\n\n\n\n\n\nc\n\nc\n\n \n\n\n\nlist:s-&gt;c:s\n\n\n\n\n\nb\n\nb\n\n \n\n\n\na:c-&gt;b:w\n\n\n\n\n\nb:c-&gt;c:w\n\n\n\n\n\nnothing\n\nnothing\n\n\n\nc:c-&gt;nothing\n\n\n\n\n\n\n\n\nFigura 3.2: Una lista ligada simple\n\n\n\n\n\nLa Figura 3.2 muestra una lista ligada, que es una implementación de lista que puede crecer fácilmente, funciona en el heap de memoria por lo que cada bloque requiere memoria dinámica. Cada bloque es una estructura; se pueden distinguir dos tipos, la lista que contiene referencias al primer nodo y al último nodo. Los nodos de de datos contienen los elementos de la colección y referencias al siguiente nodo, también llamado sucesor. El nodo nothing es especial y significa que no hay más elementos.\nEl siguiente código muestra como la definición de lista ligada.\n\n\n\n\nListado 3.1: Código para una lista ligada simple\n\n\nstruct Node\n  data::Int\n  next::Union{Node,Nothing}\nend\n\nnode = Node(10, Node(20, Node(30, nothing)))\n\nprintln(node)\n(node.data, node.next.data, node.next.next.data)\n\n\n\n\nNode(10, Node(20, Node(30, nothing)))\n\n\n(10, 20, 30)\n\n\nEn el Listado 3.1 se ignora la referencia a tail (head se guarda en node), por lo que las operaciones sobre tail requieren recorrer la lista completa, costando \\(O(n)\\) en el peor caso para una lista de \\(n\\) elementos.\nPor su manera en la cual son accedidos los datos, se tienen dos tipos de listas muy útiles: las colas y las pilas. Las colas son listas que se acceden solo por sus extremos, y emulan la política de el primero en entrar es el primero en salir (first in - first out, FIFO), y es por eso que se les llama colas haciendo referencia a una cola para realizar un trámite o recibir un servicio. Las pilas o stack son listas con la política el último en entrar es el primero en salir (last in - first out, LIFO). Mientras que cualquier lista puede ser útil para implementarlas, algunas maneras serán mejores que otras dependiendo de los requerimientos de los problemas siendo resueltos; sin embargo, es importante recordar sus políticas de acceso para comprender los algoritmos que las utilicen.\nEntre las operaciones comunes tenemos las siguientes:\n\npush!(L, a): insertar \\(a\\) al final de la lista \\(L\\).\npop!(L): remueve el último elemento en \\(L\\).\ndeleteat!(L, pos): remueve el elemento en la posición \\(pos\\), se desplazan los elementos.\ninsert!(L, pos, valor): inserta \\(valor\\) en la posición \\(pos\\) desplazando los elementos anteriores.\n\n\n3.5.0.1 Ejercicios\n\nImplemente insert! y deleteat!\n¿Cuál sería la implementación de succ y pred en una lista ligada?\n¿Cuales serían sus costos?\nAñadiendo más memoria, como podemos mejorar pred?\n\n\n\n3.5.1 Grafos\nOtras estructuras de datos elementales son los grafos. Un grafo \\(G = (V, E)\\) es una tupla compuesta por un conjunto de vertices \\(V\\) y el conjunto de aristas \\(E\\). Por ejemplo, el grafo con \\(A = (\\{a, b, c, d\\}, \\{(a, b), (b, c), (c, d), (d, a)\\})\\)\n\n\n\n\n\n\n\n\nlista\n\n\n\na\n\na\n\n\n\nb\n\nb\n\n\n\na-&gt;b\n\n\n\n\n\nc\n\nc\n\n\n\nb-&gt;c\n\n\n\n\n\nd\n\nd\n\n\n\nc-&gt;d\n\n\n\n\n\nd-&gt;a\n\n\n\n\n\n\n\n\nFigura 3.3: Un grafo dirigido simple\n\n\n\n\n\nLos grafos son herramientas poderosas para representar de manera abstracta problemas que implican relaciones entre elementos. En algunos casos es útil asociar funciones a los vértices y las aristas. Tenga en cuenta los siguientes ejemplos:\n\n\\(peso: V \\rightarrow \\mathbb{R}\\), la cual podría usarse como \\(peso(a) = 1.5\\).\n\\(costo: V \\times V \\rightarrow \\mathbb{R}\\), la cual podría usarse como \\(costo(a, b) = 2.0\\).\n\nLa estructura del grafo puede accederse mediante las funciones:\n\n\\(in(G, v) = \\{ u \\mid (u, v) \\in E\\}\\)\n\\(out(G, u) = \\{ v \\mid (u, v) \\in E\\}\\)\n\nasí como el número de vertices que entran y salen como:\n\n\\(indegree(G, v) = |in(G, v)|\\).\n\\(outdegree(G, u) = |out(G, u)|\\).\n\nUn grafo puede tener aristas no dirigidas, el grafo con \\(B=(\\{a, b, c, d\\}, \\{\\{a, b\\}, \\{b, c\\}, \\{c, d\\}, \\{d, a\\}\\})\\), no reconocerá orden en las aristas.\n\n\n\n\n\n\n\n\nlista\n\n\n\na\n\na\n\n\n\nb\n\nb\n\n\n\na--b\n\n\n\n\nc\n\nc\n\n\n\nb--c\n\n\n\n\nd\n\nd\n\n\n\nc--d\n\n\n\n\nd--a\n\n\n\n\n\n\n\nFigura 3.4: Un grafo cuyas aristas no estan dirigidas\n\n\n\n\n\nPor lo tanto, podremos decir que \\((a, b) \\in E_A\\) pero \\((b, a) \\not\\in E_A\\). Por otro lado tenemos que \\(\\{a, b\\} \\in E_B\\), y forzando un poco la notación, \\((a, b) \\in E_B\\), \\((b, a) \\in E_B\\); para los conjuntos de aristas de \\(A\\) y \\(B\\). La estructura puede ser accedida mediante \\(neighbors(G, u) = \\{ v \\mid \\{u, v\\} \\in E \\}\\).\nUn grafo puede estar representado de diferentes maneras, por ejemplo, un arreglo bidimensional (matriz), donde \\(S_{ij} = 1\\) si hay una arista entre los vértices \\(i\\) y \\(j\\); y \\(S_{ij} = 0\\) si no existe una arista. A esta representación se le llama matriz de adjacencia. Si el grafo tiene pocos \\(1\\)’s vale la pena tener una representación diferente; este es el caso de las listas de adjacencia, donde se representa cada fila o cada columna de la matriz de adjacencia como una lista de los elementos diferentes de cero.\nExisten otras representaciones como la lista de coordenadas, coordinate lists (COO), o las representaciones dispersas compimidas, sparse row (CSR) y compressed sparse column (CSC) (Scott y Tůma 2023). Todas estas representaciones tratan de disminuir el uso de memoria y aprovechar la gran dispersión para realizar operaciones solo cuando sea estrictamente necesario.\nUn árbol es un grafo en el cual no existen ciclos, esto es, no existe forma que en una caminata sobre los vértices, a traves de las aristas y prohibiendo regresarse aristas, es imposible regresar a un vértice antes visto.\n\n\n\n\n\n\n\n\nlista\n\n\n\na\n\na\n\n\n\nb\n\nb\n\n\n\na--b\n\n\n\n\nd\n\nd\n\n\n\na--d\n\n\n\n\nc\n\nc\n\n\n\nb--c\n\n\n\n\ne\n\ne\n\n\n\nd--e\n\n\n\n\nf\n\nf\n\n\n\nd--f\n\n\n\n\n\n\n\nFigura 3.5: Árbol con aristas no dirigidas\n\n\n\n\n\nEn algunos casos, es conveniente identificar vértices especiales en un árbol \\(T=(V, E)\\). Un vértice es la raíz del árbol, \\(root(T)\\), es especial ya que seguramente se utilizará como acceso al árbol y por tanto contiene un camino a cada uno vértices en \\(V\\). Cada vértice puede tener, o no, hijos \\(children(T, u) = \\{ v \\mid (u, v) \\in E \\}\\). Se dice que \\(u\\) es un hoja (leaf) si \\(children(T, u) = \\emptyset\\), e interno (inner) si no es ni raíz ni hoja.\n\n\n\n\n\n\n\n\nlista\n\n\n\na\n\na\n\n\n\nb\n\nb\n\n\n\na-&gt;b\n\n\n\n\n\nd\n\nd\n\n\n\na-&gt;d\n\n\n\n\n\nc\n\nc\n\n\n\nb-&gt;c\n\n\n\n\n\ne\n\ne\n\n\n\nd-&gt;e\n\n\n\n\n\nf\n\nf\n\n\n\nd-&gt;f\n\n\n\n\n\n\n\n\nFigura 3.6: Árbol con aristas dirigidas, note que es fácil saber si hay un vértice o nodo que se distinga como raíz, o nodos que sean hojas.\n\n\n\n\n\nAl igual que en los grafos más generales, en los árboles es útil definir funciones sobre vértices y aristas, así como marcar tipos de vértices, e.g., posición u color, que simplifiquen el razonamiento para con los algoritmos asociados.\nLos nodos y las aristas de un grafo pueden recorrerse de diferentes maneras, donde se aprovechan las relaciones representadas. En un grafo general podría ser importante solo visitar una vez cada vértice, o guiarse en el recorrido por alguna heurística o función asociada a vértices o aristas.\nEl recorrido primero a lo profundo, Depth First Search (DFS), comienza en un nodo dado y de manera voraz avanzará recordando orden de visita y avanzando al ver un nuevo nodo repitiendo el procedimiento hasta que todos los vértices alcanzables sean visitados. El siguiente pseudo-código lo implementa:\n#| lst-label: lst-dfs\n#| lst-cap: Psudo-código DFS\n\nfunction operación!(vértice)\n  #... operaciones sobre el vértice siendo visitado ...\nend\n\nfunction DFS(grafo, vértice, visitados)\n  operación!(vértice)\n  push!(visitados, vértice)\n  for v in neighbors(grafo, vértice)\n    if v ∉ visitados\n      operación!(v)\n      push!(visitados, v)\n      DFS(grafo, v, visitados)\n    end\n  end\nend\n\n# ... código de preparación del grafo\nvisitados = Set()\nDFS((vértices, aristas), vérticeinicial, visitados)\n# ... código posterior a la visita DFS\nLas llamadas recursivas a DFS tienen el efecto de memorizar el orden de visita anterior y recordarlo cuando se sale de una visita anidada. Entonces, hay una memoria implicita utilizada, implementanda por el stack de llamadas. La función operación! es una abstracción de cualquier cosa que deba hacerse sobre los nodos siendo visitados.\nEl recorrido a lo ancho, Breadth First Search (BSF), visita los vértices locales primero que los alejados contrarío al avance voraz utilizado por DFS.\n#| lst-label: lst-bfs\n#| lst-cap: Psudo-código BFS\n\nfunction BFS(grafo, vértice, visitados, cola)\n  operación!(vértice)\n  push!(visitados, vértice)\n  push!(cola, vértice)\n\n  while length(cola) &gt; 0\n    u = popfirst!(cola)\n    for v in neighbors(grafo, u)\n      if v ∉ visitados\n        operación!(v)\n        push!(visitados, v)\n        push!(cola, v)\n      end\n    end\n  end\nend\n\n# ... código de preparación del grafo\nvisitados = Set()\nBFS((vértices, aristas), vérticeinicial, visitados)\n# ... código posterior a la visita BFS\nEl BFS hace uso explícito de la memoria para guardar el orden en que se visitarán los vértices (cola); se utiliza un conjunto para marcar vértices ya visitados (visitados) con la finalidad de evitar un recorrido infinito.\n\n3.5.1.1 Ejercicios\n\nImplemente un grafo dirigido mediante listas de adyacencia.\nImplemente un grafo no dirigido mediante lista de adyacencia.\nImplemente el algoritmo de recorrido DFS y BFS con implementaciones de grafos.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estructuras de datos elementales</span>"
    ]
  },
  {
    "objectID": "cap3-estructuras.html#actividades",
    "href": "cap3-estructuras.html#actividades",
    "title": "3  Estructuras de datos elementales",
    "section": "3.6 Actividades",
    "text": "3.6 Actividades\nImplementar los siguientes algoritmos sobre matrices. - Multiplicación de matrices - Eliminación gaussiana / Gauss-Jordan Compare los desempeños de ambos algoritmos contando el número de operaciones y el tiempo real para matrices aleatorias de tamaño ( n n ) para ( n= 100, 300, 1000). Maneje de manera separada los datos de conteo de operaciones (multiplicaciones y sumas escalares) y las de tiempo real. Discuta sus resultados experimentales; ¿qué puede concluir? ¿Cuál es el impacto de acceder los elementos contiguos en memoria de una matriz? ¿Qué cambiaría si utiliza matrices dispersas? ¿Cuáles serían los costos?\nEntregable\nSu trabajo se entregará en PDF y con el notebook fuente; deberá estar plenamente documentado, con una estructura que permita a un lector interesado entender el problema, sus experimentos y metodología, así como sus conclusiones. Tenga en cuenta que los notebooks pueden alternar celdas de texto y código.\nNo olvide estructurar su reporte, en particular el reporte debe cubrir los siguientes puntos:\n\nTítulo del reporte, su nombre.\nIntroducción.\nCódigo cercano a la presentación de resultados.\nFiguras y tablas\nAnálisis de los resultados\nConclusión, discusiones de las preguntas\nLista de referencias. Nota, una lista de referencias que no fueron utilizadas en el cuerpo del texto será interpretada como una lista vacía.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estructuras de datos elementales</span>"
    ]
  },
  {
    "objectID": "cap3-estructuras.html#bibliografía",
    "href": "cap3-estructuras.html#bibliografía",
    "title": "3  Estructuras de datos elementales",
    "section": "3.7 Bibliografía",
    "text": "3.7 Bibliografía\nCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2022). Introduction to Algorithms (2nd ed.). MIT Press.\n\nParte III: Cap 10 Elementary Data Structures.\nParte VI: Cap 22 Elementary Graph Algorithms.\nParte VII: Cap 28 Matrix Operations.\n\n\n\n\n\n\n\nScott, Jennifer, y Miroslav Tůma. 2023. “An Introduction to Sparse Matrices”. En Algorithms for Sparse Linear Systems, 1–18. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-25820-6_1.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estructuras de datos elementales</span>"
    ]
  },
  {
    "objectID": "cap4-ordenamiento.html",
    "href": "cap4-ordenamiento.html",
    "title": "4  Algoritmos de ordenamiento",
    "section": "",
    "text": "Objetivo\nImplementar y analizar algoritmos de ordenamiento de arreglos con costo óptimo en el peor caso, así como algoritmos adaptativos a la entrada para caracterizar su desempeño bajo un enfoque experimental para la solución efectiva de problemas informáticos.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Algoritmos de ordenamiento</span>"
    ]
  },
  {
    "objectID": "cap4-ordenamiento.html#introducción",
    "href": "cap4-ordenamiento.html#introducción",
    "title": "4  Algoritmos de ordenamiento",
    "section": "4.1 Introducción",
    "text": "4.1 Introducción\nEn este tema se aborda el ordenamiento basado en comparación, esto es, existe un operador \\(&lt;\\) que es capaz de distinguir si un elemento \\(a\\) es menor que un elemento \\(b\\).\nEl operador cumple con las siguientes propiedades:\n\nsi \\(a &lt; b\\) y \\(b &lt; c\\) entonces \\(a &lt; c\\) (transitividad); e.g., \\(1 &lt; 10\\) y \\(10 &lt; 100\\) entonces \\(1 &lt; 100\\).\ntricotomía:\n\nsi \\(a &lt; b\\) es falso y \\(b &lt; a\\) es falso, entonces \\(a = b\\) (antisímetria); dicho de otras formas:\n\nsi \\(a\\) no es menor que \\(b\\) ni \\(b\\) menor que \\(a\\) entonces \\(a\\) es igual a \\(b\\),\ndesvelando variables, \\(1 &lt; 1\\) es falso, el intercambio es obvio, entonces \\(1=1\\).\n\nen otro caso, \\(a &lt; b\\) o \\(a &lt; b\\).\n\n\n\n\nUsar un operador como \\(&lt;\\) es suficiente para crear algoritmos correctos y eficientes, sin embargo, en la práctica y en una computadora real, también es válido utilizar operadores como \\(=\\) o \\(\\leq\\), o intercambiar por \\(&gt;\\) y \\(\\geq\\) según convenga. No hay impacto en la eficiencia.\nSin perdida de generalidad, podemos planter el problema de ordenamiento sin permitir repeticiones como sigue: dado un arreglo \\(A[1, n] = a_1, a_2, \\cdots, a_n\\); un algoritmo de ordenamiento obtiene la permutación \\(\\pi\\) tal que \\(a_{\\pi(1)} &lt; a_{\\pi(2)} &lt; \\cdots &lt; a_{\\pi(n)}\\).\n\n\nCuando se permiten elementos repetidos, se le llama ordenamiento estable i se asegura que en el arreglo ordenado se preserven el orden original posicional cuando \\(a = b\\). Esta propiedad es importante cuando hay datos satélitales asociados a la llave de comparación.\nEn términos prácticos, la idea es reorganizar \\(A\\), mediante el cálculo implicito de la permutación \\(\\pi\\), de tal forma que después de terminar el proceso de ordenamiento se obtenga que \\(A\\) esta ordenado, i.e., \\(a_i \\leq a_{i+1}\\). En sistemas reales, el alojar memoria para realizar el ordenamiento implica costos adicionales, y es por esto muchas veces se busca modificar directamente \\(A\\).\n\n\nUtilizar \\(\\pi\\) solo es necesario cuando no es posible modificar \\(A\\). También es muy común utilizar datos satélite asociados con los valores a comparar, de esta manera es posible ordenar diversos tipos de datos. Un ejemplo de esto es ordenar un dataframe, pero también estructuras de datos donde existe un campo especial y el resto de los datos asociados es de importancia para una aplicación.\n\n4.1.1 Costo del problema\nPara una entrada de tamaño \\(n\\) existen \\(n!\\) permutaciones posibles; cada una de estas permutaciones es una instancia del problema de ordenamiento de tamaño \\(n\\).\nExiste una permutación objetivo \\(\\pi^*\\), i.e., que cumple con la definición de que esta ordenada; ahora pensemos en un grafo donde cada \\(\\pi_i\\) esta conectada con todas las permutaciones en las que se puede transformar haciendo una única operación, e.g., intercambiando un elemento. El algoritmo forma ese grafo con sus posibles decisiones, por lo que el camino más largo i.e., ruta sin ciclos, entre cualquier \\(\\pi_i\\) y la permutación \\(\\pi^*\\) es el costo de peor caso del algoritmo.\nAhora, cada operación que realicemos en un algoritmo nos acercará más a \\(\\pi^*\\), descartando una cierta cantidad de instancias posibles pero no viables; si nuestra función de transición en el grafo viene dada con respecto a colocar cada par de elementos en su orden relativo, entonces, la mitad de las permutaciones se han descartado, ya que ese par no puede estar en el orden contrario. Por tanto, el costo de cualquier algoritmo que realice comparaciones y descarte la mitad del espacio de búsqueda, es \\(\\log_2(n!)\\), que usando la aproximación de Stirling,1 lo podemos reescribir como sigue:\n\\[\\log_2(n!) = n \\log_2 n - n \\log_2 e + O(\\log_2 n)\\]\nEsto se puede simplemente escribir como \\(O(n \\log n)\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Algoritmos de ordenamiento</span>"
    ]
  },
  {
    "objectID": "cap4-ordenamiento.html#algoritmos-de-ordenamiento",
    "href": "cap4-ordenamiento.html#algoritmos-de-ordenamiento",
    "title": "4  Algoritmos de ordenamiento",
    "section": "4.2 Algoritmos de ordenamiento",
    "text": "4.2 Algoritmos de ordenamiento\nExisten muchos algoritmos que pueden resolver el problema de ordenamiento, es común contar el número de comparaciones ya que produce la información necesaria para la navegación en el grafo de instancias; también es común contar las operación de intercambiar elementos. Las pruebas y la navegación en el grafo determina el costo del algoritmo. Es necesario mencionar que mover datos entre diferentes zonas de memoria puede llegar a ser más costoso que solo acceder a esas zonas por lo que hay una asimetría en el costo de estas dos operaciones.\nNote que algunos de los algoritmos más simples pueden tener un comportamiento oportunistas y que son capaces de obtener ventaja en instancias sencillas, por lo que no debería saltarse esas secciones si solo conoce su comportamiento en peor caso.\n\n4.2.1 Bubble sort\nEl algoritmo de ordenamiento de burbuja o bubble sort realiza una gran cantidad de comparaciones, como puede verse en Listado 4.1, el algoritmo usa dos ciclos anidados para realizar una comparación y una posible transposición, formando un triángulo, i.e., \\[ \\sum_{i=1}^{n-1} \\sum_{j=1}^{n-i} O(1);\\] por lo tanto su costo esta dominado por el triangulo formado, i.e., \\(\\sim n^2/2\\) lo que puede escribirse simplemente como \\(O(n^2)\\).\n\n\n\n\nListado 4.1: Bubble sort de peor caso\n\n\nfunction bubble_sort!(A)\n  n = length(A)\n  for i in 1:n-1\n    for j in 1:n-i\n      if A[j] &gt; A[j+1]\n        A[j], A[j+1] = A[j+1], A[j]\n      end\n    end\n  end\n  \n  A\nend\n\nbubble_sort!([8, 4, 3, 1, 6, 5, 2, 7])\n\n\n\n\n8-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n\n\n\nCiclo que recorre \\(n-1\\) veces todo el arreglo; y pone el elemento máximo en su posición final.\nCiclo que recorre \\(n-i\\) veces el arreglo; ya que en cada corrida se pone el máximo en su posición.\nIntercambio cuando hay pares en desorden.\n\nEl algoritmo mostrado en Listado 4.1 es un algoritmo de peor caso, ya que sin importar la complejidad de la instancia (i.e., que tal alejada esta \\(\\pi_i\\) de \\(\\pi^*\\)), se comporta igual.\nEs relativamente fácil hacer un bubble sort que tenga en cuenta la complejidad de la instancia, medida como el número de intercambios necesarios.\n\n\n\n\nListado 4.2: Bubble sort adaptable\n\n\nfunction adaptive_bubble_sort!(A)\n  n = length(A)\n\n  for i in 1:n-1     \n    s = 0            \n    for j in 1:n-i\n      if A[j] &gt; A[j+1]\n        s += 1\n        A[j], A[j+1] = A[j+1], A[j] \n      end\n    end\n    s == 0 && break\n  end\n  \n  A\nend\n\nadaptive_bubble_sort!([7, 8, 4, 3, 1, 6, 5, 2])\n\n\n\n\n8-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n\n\n\nLa idea es que si no hay intercambios en una iteración, entonces el arreglo ya esta ordenado.\nContador de intercambios.\nCondición de paro, i.e., no hubo intercambios.\n\nEn la forma Listado 4.2, bubble sort es capaz de términar en \\(n-1\\) comparaciones si el arreglo esta ordenado; sacando provecho de casos simples en términos de instancias casi ordenadas.\n\n\n4.2.2 Insertion sort\nEl algoritmo de ordenamiento por inserción o insertion sort es un algoritmo simple que al igual que bubble sort tiene un mal peor caso y puede aprovechar casos simples\n\n\n\n\nListado 4.3: Algoritmo insertion sort\n\n\nfunction insertion_sort!(A)\n  n = length(A)\n  for i in 2:n\n    key = A[i]\n    j = i - 1   \n    while j &gt;= 1 && A[j] &gt; key\n      A[j + 1] = A[j]\n      j -= 1\n    end\n\n    A[j + 1] = key\n  end\n  \n  A\nend\n\ninsertion_sort!([5, 1, 4, 8, 2, 6, 3, 7])\n\n\n\n\n8-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n\n\n\nEl algoritmo comienza en la segunda posición del arreglo y revisará todos los elementos.\nEs importante hacer una copia de key para simplificar la implementación.\nLa idea general es ordenar las posiciones de \\(1..i\\), para esto se debe recorrer hacia atrás el arreglo completo, para determinar la posición de inserción de key.\nIntercambio de elementos para colocar key en su lugar ordenado.\nkey se pone en su lugar final.\n\nPara analizar Listado 4.3, es importante notar que el ciclo más externo termina con el subarreglo \\(A[1..i]\\) ordenado; por lo que cuando se comienza el ciclo, si key se prueba estar en su posición correcta, entonces ya no es necesario revisar el resto del subarreglo, esto determina que un arreglo ordenado tendrá un costo de \\(O(n)\\) comparaciones; si esta casi ordenado en términos del número de intercambios necesarios, entonces, el algoritmo se adaptará sacando provecho de la instancia.\nEn el peor caso de insertion sort, el algoritmo no puede parar de manera prematura, e.g., un arreglo en orden reverso, el ciclo for se ejecutara \\(n-1\\) veces, mientras que el ciclo while deberá revisar el subarreglo completo en cada iteración, sumando un costo de \\(i\\) operaciones en cada iteración, i.e., \\(\\sum_{i=1}^n i\\), esta forma produce un triángulo, resultando en un costo \\(O(n^2)\\).\n\n\n4.2.3 Quick sort\nQuick sort (ver Cormen et al. 2022, cap. 7) es un algoritmo tipo dividir para vencer; esto es, un algoritmo que divide un problema grande en instancias pequeñas más sencillas. Es uno de los algoritmos más veloces en la práctica por su buen manejo de memoria, aun cuando tiene un peor caso cuadrático, en promedio el costo es \\(O(n \\log n)\\).\n\n\n\n\nListado 4.4: Algoritmo quick sort.\n\n\nusing Random\n\nfunction qsort!(A, low=1, high=length(A))\n  if low &lt; high\n      piv = part!(A, low, high)\n      qsort!(A, low, piv - 1)\n      qsort!(A, piv + 1, high)\n  end\n  \n  A\nend\n\nfunction part!(A, low, high)\n  ipiv = rand(low:high)\n  A[ipiv], A[high] = A[high], A[ipiv]\n  piv = A[high]\n\n  i = low - 1  # uno antes porque se accede después de un i+1\n  for j in low:high - 1\n      if A[j] &lt; piv\n          i += 1\n          A[i], A[j] = A[j], A[i]\n      end\n  end\n  \n  ipiv = i + 1\n  A[ipiv], A[high] = A[high], A[ipiv]\n  ipiv\nend\n\nqsort!([6, 8, 3, 7, 4, 1, 2, 5])\n\n\n\n\n8-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n\n\n\nEl arreglo se divide en 3 partes, ordenadas entre sí, un subarreglo izquierdo, un pivote, y un subarreglo derecho; los subarreglos no estan ordenados localmente, pero el pivote esta en su posición final.\nSe resuelve el problema izquierdo y el problema derecho por separado.\nLa función part! particiona el arreglo \\(A[low:end]\\) en 3 partes como se específico en el punto 1; para eso selecciona de manera aleatoria un pivote. Lo ponemos al final del arreglo para simplificar el código siguiente.\nEste ciclo itera por todo el subarreglo, su objetivo es asegurar que \\(A[i] &lt; piv\\) para todo \\(i \\in low:piv-1\\) y \\(piv &lt; A[i]\\) para todo \\(i \\in piv+1:high\\).\nIntercambia elementos si \\(A[j] &lt; piv\\), hacemos seguimiento de \\(i\\) ya que esta posición determinará al pivote.\nComo piv se encontraba en high, entonces hay que intercambiarlos para que qsort! sepa como manejarlos; recordando que los subarreglos no estan ordenados dentro de sí.\n\nEl código Listado 4.4 es relativamente simple, usa recurrencias sobre qsort! sobre dos partes extremas divididas por un pivote; estos tres elementos son encontrados en part!. La función part! es muy eficiente en términos de memoria, lo que puede hacer la diferencia en la práctica. La correcta selección del pivote es muy importante para evitar casos malos, i.e., costo cuadrático; en esta implementación se realiza una selección aleatoría de pivote que funcionará en la mayoría de los casos.\nEl peor de los casos en qsort! es debido a una mala selección del pivote, de tal forma que \\[|A[low:piv-1]| \\ll |A[piv+1:high]|,\\] o lo contrario en toda selección, en el extremo una de los subarreglos puede verse como de tamaño constante o cero, i.e., selección de pivote como el minimo o el máximo. Esta estrategía reduce a qsort! a un costo \\(O(n^2)\\).\nSi se realiza un particionado donde \\[|A[low:piv-1]| \\approx |A[piv+1:high]|,\\] entonces tenemos un algoritmo \\(O(n \\log n)\\); ya que hace una división en dos partes casi iguales en cada recurrencia a qsort!, y esto solo puede profundizar a \\(\\log n\\) veces, y en cada nivel part! tiene un costo lineal.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Algoritmos de ordenamiento</span>"
    ]
  },
  {
    "objectID": "cap4-ordenamiento.html#skip-list",
    "href": "cap4-ordenamiento.html#skip-list",
    "title": "4  Algoritmos de ordenamiento",
    "section": "4.3 Skip list",
    "text": "4.3 Skip list\nUna skip list (Pugh 1990) es una lista ligada con capacidades de búsqueda eficiente con garantías probabilísticas, esto es que se cumplen con alta probabilidad. Para esto, la idea es que cada dato tiene asociado un arreglo de punteros o referencias hacia nodos sucesores, i.e., los nodos a nivel \\(i\\) se conectan con el siguiente nodo a nivel \\(i\\). En el nivel más bajo, la skip list es una simple lista ligada, mientras que sube, se vuelve más dispersa dando saltos más largos.\n\n\n\n\n\n\n\n\nlista\n\n\n\nhead\n\n(4)\n\n(3)\n\n(2)\n\n(1)\n\n(head)\n\n\n\na\n\n(2)\n\n(1)\n\na\n\n\n\nhead:1-&gt;a:1\n\n\n\n\n\nhead:2-&gt;a:2\n\n\n\n\n\n\ne\n\n(3)\n\n(2)\n\n(1)\n\ne\n\n\n\nhead:3-&gt;e:3\n\n\n\n\n\nh\n\n(4)\n\n(3)\n\n(2)\n\n(1)\n\nh\n\n\n\nhead:e-&gt;h:w\n\n\n\n\n\nb\n\n(1)\n\nb\n\n\n\na:1-&gt;b:1\n\n\n\n\n\n\nc\n\n(2)\n\n(1)\n\nc\n\n\n\na:2-&gt;c:2\n\n\n\n\n\nb:1-&gt;c:1\n\n\n\n\n\n\nd\n\n(1)\n\nd\n\n\n\nc:1-&gt;d:1\n\n\n\n\n\n\nc:2-&gt;e:2\n\n\n\n\n\nd:1-&gt;e:1\n\n\n\n\n\n\nf\n\n(1)\n\nf\n\n\n\ne:1-&gt;f:1\n\n\n\n\n\n\ng\n\n(2)\n\n(1)\n\ng\n\n\n\ne:2-&gt;g:2\n\n\n\n\n\ne:3-&gt;h:3\n\n\n\n\n\nf:1-&gt;g:1\n\n\n\n\n\n\ng:1-&gt;h:1\n\n\n\n\n\ng:2-&gt;h:2\n\n\n\n\n\n\ni\n\n(1)\n\ni\n\n\n\nh:1-&gt;i:1\n\n\n\n\n\n\nj\n\n(3)\n\n(2)\n\n(1)\n\nj\n\n\n\nh:2-&gt;j:2\n\n\n\n\n\nh:3-&gt;j:3\n\n\n\n\n\ntail\n\n(4)\n\n(3)\n\n(2)\n\n(1)\n\n(tail)\n\n\n\nh:e-&gt;tail:w\n\n\n\n\n\ni:1-&gt;j:1\n\n\n\n\n\n\nj:1-&gt;tail:1\n\n\n\n\n\nj:2-&gt;tail:2\n\n\n\n\n\nj:3-&gt;tail:3\n\n\n\n\n\n\n\n\n\nFigura 4.1: Ejemplo de una skip list\n\n\n\n\n\nA diferencia de los algoritmos vistos anteriormente, en este caso, ya se tiene una estructura de datos, que conlleva un costo en memoría explícito por nodo. Figura 4.1 ilustra la estructura.\nLa altura de cada nodo es calculada de manera probabilística, dada la probababilidad \\(p\\). Un valor común de \\(p=0.5\\). La altura de cada nodo se calcula como sigue:\n\nfunction levels(p)\n  i = 1\n  while rand() &lt; p\n    i += 1\n  end\n\n  i\nend\n\nlevels (generic function with 1 method)\n\n\nSi tenemos \\(n\\) evaluaciones de levels, Los niveles pequeños son relativamente probables, mientras que niveles grandes son relativamente poco probables. De hecho, los niveles \\(\\log_{1/p} n\\) son cercanos a una constante, \\(\\log_{1/p}{n} - 1\\) son \\(1/p\\) veces la constante, \\(\\log_{1/p}{n} - 2\\) son \\(1/p^2\\) veces la constante, etc.\nA diferencia de los algoritmos anteriores, una skip list comienza vacia, y se va poblando insertando elementos a la lista. Se va colocando en la posición que no viola el orden; generando el nodo correspondiente con nivel calculado. Los nodos especiales head y tail siempre tienen el nivel máximo posible. La inserción de un valor encapsulado en el nodo \\(u\\) comienza por visitar el máximo nivel en head e ir bajando hasta determinar \\(u.dato &gt; head[level].dato\\); en ese momento se debe avanzar al nodo apuntado por \\(head[level]\\) y repetir el algoritmo hasta que \\(level=1\\), en cuyo caso encontramos el lugar de inserción del nuevo dato. Se procede a reasignar los punteros de los sucesores y ajustar los punteros hacia los nodos sucesores a los niveles que tiene \\(u\\).\nCada inserción tiene un costo \\(O(\\log_{1/p} n)\\), garantía probabilística; por lo que insertar \\(n\\) elementos tiene un costo: \\[ \\sum_{i=1}^n O(\\log_{1/p} i) = O(\\log_{1/p} \\prod_{i=1}^n i) = O(\\log_{1/p} {n!}) = O(n \\log n); \\] usando la aproximación de Stirling.\nA diferencia de la versión basada en arreglos, una skip list es capaz de aceptar nuevos elementos y mantener el orden de manera eficiente.\n\n4.3.1 Ejercicios:\n\nInvestigue, implemente y pruebe merge sort. 1.1 ¿Cuales son las ventajas y desventajas de merge sort? 1.2 ¿Por qué merge sort se puede utilizar en algoritmos paralelos y otros pueden tener muchas dificultades? 1.3 ¿Cómo se puede reducir la memoria extra necesaria de merge sort?\nInvestigue, implemente y pruebe heap sort. 2.1 ¿Cuales son las ventajas y desventajas de heap sort?\n¿Cuál es el costo en memoria de una skip list?. 3.1 Investigue, implemente y pruebe un skip list.\nInvestigue, implemente y pruebe un árbol binario de búsqueda.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Algoritmos de ordenamiento</span>"
    ]
  },
  {
    "objectID": "cap4-ordenamiento.html#lecturas",
    "href": "cap4-ordenamiento.html#lecturas",
    "title": "4  Algoritmos de ordenamiento",
    "section": "4.4 Lecturas",
    "text": "4.4 Lecturas\nLas lecturas de este tema corresponden al capítulo 5 de (Knuth 1998), en específico 5.2 Internal sorting. También se recomienda leer y comprender la parte II de (Cormen et al. 2022), que corresponde a Sorting and order statistics, en partícular Cap. 6 y 7, así como el Cap. 8.1. El artículo de wikipedia https://en.wikipedia.org/wiki/Sorting_algorithm también puede ser consultado con la idea de encontrar una explicación rápida de los algoritmos.\nEn la práctica, pocos algoritmos son mejores que quicksort. En (Loeser 1974) se detalla una serie de experimentos donde se compara quicksort contra otros algoritmos relacionados; por lo que es una lectura recomendable.\nLa parte adaptable, esto es para algoritmos oportunistas que toman ventaja de instancias simples, esta cubierta por el artículo (Estivill-Castro y Wood 1992). En especial, es muy necesario comprender las secciones 1.1 y 1.2, el resto del artículo debe ser leído aunque no invierta mucho tiempo en comprender las pruebas expuestas si no le son claras. En especial, en las secciones indicadas se establecen las medidas de desorden contra las cuales se mide la complejidad. En (Cook y Kim 1980) realiza una comparación del desempeño de varios algoritmos para ordenamiento de listas casi ordenadas, esto es, en cierto sentido donde los algoritmos adaptables tienen sentido. Este artículo es anterior a (Estivill-Castro y Wood 1992) pero tiene experimentos que simplifican el entendimiento de los temas.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Algoritmos de ordenamiento</span>"
    ]
  },
  {
    "objectID": "cap4-ordenamiento.html#material-audio-visual-sobre-algoritmos-de-ordenamiento",
    "href": "cap4-ordenamiento.html#material-audio-visual-sobre-algoritmos-de-ordenamiento",
    "title": "4  Algoritmos de ordenamiento",
    "section": "4.5 Material audio-visual sobre algoritmos de ordenamiento",
    "text": "4.5 Material audio-visual sobre algoritmos de ordenamiento\n\n\n\n\n\n\n\n\nCook, Curtis R, y Do Jin Kim. 1980. “Best sorting algorithm for nearly sorted lists”. Communications of the ACM 23 (11): 620–24.\n\n\nCormen, Thomas H, Charles E Leiserson, Ronald L Rivest, y Clifford Stein. 2022. Introduction to algorithms. MIT press.\n\n\nEstivill-Castro, Vladmir, y Derick Wood. 1992. “A survey of adaptive sorting algorithms”. ACM Computing Surveys (CSUR) 24 (4): 441–76.\n\n\nKnuth, Donald. 1998. The Art Of Computer Programming, vol. 3 (2nd ed): Sorting And Searching. Vol. 3. Redwood City, CA, USA.: Addison Wesley Longman Publishing Co. Inc.\n\n\nLoeser, Rudolf. 1974. “Some performance tests of ‘quicksort’ and descendants”. Communications of the ACM 17 (3): 143–52.\n\n\nPugh, William. 1990. “Skip lists: a probabilistic alternative to balanced trees”. Commun. ACM 33 (6): 668–76. https://doi.org/10.1145/78973.78977.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Algoritmos de ordenamiento</span>"
    ]
  },
  {
    "objectID": "cap4-ordenamiento.html#footnotes",
    "href": "cap4-ordenamiento.html#footnotes",
    "title": "4  Algoritmos de ordenamiento",
    "section": "",
    "text": "Aproximación de Stirling https://en.wikipedia.org/wiki/Stirling%27s_approximation.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Algoritmos de ordenamiento</span>"
    ]
  },
  {
    "objectID": "cap5-busqueda.html",
    "href": "cap5-busqueda.html",
    "title": "5  Algoritmos de búsqueda en el modelo de comparación",
    "section": "",
    "text": "Objetivo\nAnalizar algoritmos de búsqueda en arreglos ordenados basados en funciones de comparación, con el objetivo de localizar elementos y posiciones específicas, usando técnicas de peor caso y adaptables a la distribución de los datos para una solución eficiente de problemas informáticos.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Algoritmos de búsqueda en el modelo de comparación</span>"
    ]
  },
  {
    "objectID": "cap5-busqueda.html#problema",
    "href": "cap5-busqueda.html#problema",
    "title": "5  Algoritmos de búsqueda en el modelo de comparación",
    "section": "5.1 Problema",
    "text": "5.1 Problema\nSea \\(A[1..n] = a_1, \\cdots, a_n\\) un arreglo ordenado con \\(n \\geq 1\\) y un operador \\(&lt;\\) (menor que); por simplicidad, también usaremos \\(\\leq\\) (menor o igual que). Supondremos que no hay elementos duplicados en \\(A\\), note que esto no implica una perdida de generalidad.\nLa tarea será: dado el valor \\(x\\) a ser localizado en \\(A\\), el problema consiste en determinar la posición de inserción \\(p\\) tal que suceda alguna de las siguientes condiciones:\n\nsi \\(p = 1\\) entonces \\(x \\leq A[p]\\).\nsi \\(2 \\leq p \\leq n\\) entonces \\(A[p-1] &lt; x \\leq A[p]\\).\nsi \\(p=n+1\\) entonces \\(A[n] &lt; x\\).\n\n\n5.1.1 Costo de peor caso\nPara \\(A[1..n]\\) y el valor \\(x\\) a localizar su posición de inserción, el resultado puede ser cualquiera de las \\(n+1\\) posiciones posibles, i.e., instancias del problema. Un algoritmo naïve utilizaría \\(n\\) comparaciones para resolverlo.\n\n\"\"\"\n    seqsearch(A, x, sp=1)\n\nBúsqueda exhaustiva con inicio\n\"\"\"\nfunction seqsearch(A, x, sp=1)\n    n = length(A)\n    while sp &lt;= n && x &gt; A[sp]\n        sp += 1\n    end\n    \n    sp\nend\n\nlet S=[10, 20, 30, 40, 50, 60, 70]\n    (seqsearch(S, 0), seqsearch(S, 69), seqsearch(S, 70), seqsearch(S, 71))\nend\n\n(1, 7, 7, 8)\n\n\nSin embargo, dado que el arreglo esta ordenado y no hay duplicados, se puede mejorar mucho el tiempo de búsqueda.\n\n\nSi se permiten duplicados se pueden mejorar muchos los tiempos; sobre todo si podemos preprocesar el arreglo, i.e., para determinar las zonas con duplicados.\nEl costo de búsqueda para cualquier instancia es \\(O(\\log n)\\), y viene de la búsqueda binaria:\n\n\"\"\"\n    binarysearch(A, x, sp=1, ep=length(A))\n\nEncuentra la posición de inserción de `x` en `A` en el rango `sp:ep`\n\"\"\"\nfunction binarysearch(A, x, sp=1, ep=length(A))\n3    while sp &lt; ep\n1        mid = div(sp + ep, 2)\n        if x &lt;= A[mid]\n2            ep = mid\n        else\n            sp = mid + 1\n        end\n    end\n    \n4    x &lt;= A[sp] ? sp : sp + 1\nend\n\nlet S=[10, 20, 30, 40, 50, 60, 70]\n    (binarysearch(S, 0), binarysearch(S, 69), binarysearch(S, 70), binarysearch(S, 71))\nend\n\n\n1\n\nPara el rango de búsqueda \\(sp:ep\\) se determina su punto central \\(mid\\) y se compara con \\(x\\),\n\n2\n\nSi el elemento \\(x\\) esta a la izquierda, se ajusta el limite superior \\(ep,\\) o de lo contrario se ajusta \\(sp\\). Ambos ajustes se hacen tomando en cuenta la posición comparada.\n\n3\n\nSe itera mientras no se junten los dos extremos del rango.\n\n4\n\nFinalmente, se ajusta para valores fuera del rango.\n\n\n\n\n(1, 7, 7, 8)\n\n\nEste algoritmo es simple y efectivo, y es capaz de resolver cualquier instancia en tiempo logarítmico, y esto lo hace al dividir el rango siempre a la mitad por cada iteración. El costo de búsqueda binaria es de \\(C_\\text{bin}(n) = \\lfloor \\log n \\rfloor + O(1)\\) comparaciones antes de colapsar el rango donde puede estar la posición de inserción.\nEs importante hacer notar que la búsqueda binaria es muy eficiente en memoría y tiene un peor caso óptimo, ya que es idéntico al costo del problema, i.e., así lo determinamos. Si fuera posible tener probar varios puntos, i.e., \\(m\\) segmentos en una sola operación, el costo estaría acotado en \\(\\lceil \\log_{m} n \\rceil\\). Esto tiene sentido para estructuras de datos que trabajan en diferentes niveles de memoría, donde aunque las comparaciones en hardware moderno sean binarias, la diferencia entre velocidades de los diferentes niveles de memoria se puede pensar que el costo dominante es, por ejemplo, acceder a una zona de disco y obtener una decisión entre \\(m-1\\) posibles, que particionan los rangos en \\(m\\) divisiones.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Algoritmos de búsqueda en el modelo de comparación</span>"
    ]
  },
  {
    "objectID": "cap5-busqueda.html#búsqueda-no-acotada",
    "href": "cap5-busqueda.html#búsqueda-no-acotada",
    "title": "5  Algoritmos de búsqueda en el modelo de comparación",
    "section": "5.2 Búsqueda no acotada",
    "text": "5.2 Búsqueda no acotada\nCuando el tamaño del arreglo es demasiado grande, o la relación entre \\(p / n\\) es significativamente pequeña, la búsqueda acotada no es la mejor opción. Aun cuando en la práctica el límite superior \\(n\\) podría estar determinado, y por lo tanto, se pueden resolver búsquedas en \\(O(\\log n)\\), es posible obtener una cota relativa a \\(p\\), independiente de \\(n\\), por lo que los casos de interés se verán beneficiados.\nUna estrategia simple y poderosa es la siguiente:\n\nDeterminar un buen rango que contenga la respuesta.\nAplicar búsqueda binaria en ese rango para obtener la respuesta.\n\nBentley y Yao (1976) describen a detalle una familia de algoritmos casí óptimos para la búsqueda no acotada siguiendo la estrategía anteriormente mencionada. En particular, poniendo un enfásis importante en la determinación del rango. Lo consigue mediante la definición de algoritmos definidos de manera interesante como sigue en el resto de la sección.\n\n5.2.1 Algoritmo \\(B_0\\) (búsqueda unaría)\nEs el algoritmo más simple, y ya lo vimos con anterioridad, realiza una búsqueda exhaustiva de la posición de inserción, hacendo pruebas para toda posición \\(x \\leq A[1], x \\leq A[2], \\cdots, x \\leq A[p+1]\\), por lo que su costo será de \\(p+1\\).\nSea \\(F_0(n)\\) una secuencia de puntos para un arreglo de longitud \\(n\\), donde se harán comparaciones para determinar el rango que contenga la respuesta para el algoritmo \\(B_0\\) y \\(C_0(p)\\) el costo de búsqueda. Entonces:\n\n\\(F_0(n) = 1, 2, \\cdots, n, n+1\\).\n\\(C_0(p) = p+1\\); no requiere búsqueda binaria.\n\n\n\n5.2.2 Algoritmo \\(B_1\\) (búsqueda doblada: doubling search/galloping)\nConsiste en comparar las posiciones \\(2^i\\), i.e., \\(2^1, 2^2, 2^3, \\cdots, 2^{\\lfloor \\log_2{p+1} \\rfloor + 1}\\), tal que \\(A[2^{\\lfloor\\log_2{p}\\rfloor+1}] \\leq  x \\leq A[2^{\\lfloor\\log_2{p+1}\\rfloor+1}]\\). De manera similar que para \\(B_0\\) definimos \\(F_1(n)\\) y \\(C_1\\):\n\n\\(F_1(n) = 2^1, 2^2, \\cdots, 2^{\\log \\lfloor n \\rfloor + 1};\\)\n\\(C_1(p) = C_\\text{bin}{(2^{\\log_2{p+1}})} + \\log_2{(p+1)} + 1 &lt; 2\\log_2 p + O(1).\\)\n\nLa explicación viene a continuación. El número de comparaciones para determinar el rango esta determinado por \\(\\lfloor \\log_2{p+1} \\rfloor + 1\\). Una vez determinado el rango la búsqueda binaria sobre \\[A[2^{\\lfloor\\log_2{p}\\rfloor+1}:2^{\\lfloor\\log_2{(p+1)}\\rfloor+1}],\\] lo cual corresponde a \\(\\log_2 2^{\\log{(p+1)}+1}/2 = \\log_2{(p+1)}\\). El costo \\(C_1(p)\\) puede ser escrito como \\(2\\log_2 p + O(1)\\), con un poco de manipulación algebraica.\nEs importante saber cuando usar un algoritmo u otro, por tanto determinar cuando \\(2\\log_2{p} + O(1) &lt; \\log_2 n + O(1).\\) Para simplificar este análisis ignoraremos algunos detalles de la expresión: \\[\\begin{align}\n2\\log_2{p} & &lt; \\log_2 n, \\\\\n2^{\\log_2{p^2}} & &lt; 2^{\\log_2 n}, \\\\\np^2              & &lt;  n, \\\\\np               & &lt;  \\sqrt{n}; \\\\\n\\end{align}\\] esto indica que si \\(p\\) es menor a \\(\\sqrt{n}\\) entonces hay una ventaja al usar \\(B_1\\); lo cual nos dice que para posiciones cercanas al inicio el uso de \\(B_1\\) puede llevar a búsquedas más veloces. Note que en la práctica es necesario tener en cuenta la memoria, interesantemente, para \\(p\\) pequeñas es posible que esto beneficie al algoritmo ya que podría mantener las listas en cache.\nEl siguiente código implementa \\(B_1\\)\n\nfunction doublingsearch(A, x, sp=1)\n    n = length(A)\n    p = 0\n    i = 1\n\n1    while sp+i &lt;= n && A[sp+i] &lt; x\n        p = i\n        i += i\n    end\n\n2    binarysearch(A, x, sp + p, min(n, sp+i))\nend\n\nlet S=[10, 20, 30, 40, 50, 60, 70]\n    (doublingsearch(S, 0), doublingsearch(S, 69), doublingsearch(S, 70), doublingsearch(S, 71))\nend\n\n\n1\n\nDeterminación del rango.\n\n2\n\nAplicar un algoritmo de búsqueda eficiente en el rango que contiene la respuesta.\n\n\n\n\n(1, 7, 7, 8)\n\n\nEs cierto que estos algoritmos son oportunistas, pero hay aplicaciones donde esto realmente sucede. En el peor caso, el costo será apenas dos veces el óptimo.\n\n\n5.2.3 Algoritmo \\(B_2\\) (búsqueda doblemente doblada, doubling-doubling search)\nAquí será más clara la dinámica. \\(B_2\\) consiste en comparar las posiciones \\(2^{2^i}\\), i.e., \\(2^{4}, 2^{16}, 2^{256}, \\cdots, 2^{2^{\\lfloor \\log_2{\\lfloor\\log_2{p+1}\\rfloor + 1} \\rfloor + 1}}\\), tal que \\[A[2^{2^{\\lfloor\\log_2{\\lfloor\\log_2{p}\\rfloor+1}\\rfloor + 1}}] \\leq  x \\leq A[2^{2^{\\lfloor\\log_2{\\lfloor\\log_2{p+1}\\rfloor+1}\\rfloor + 1}}];\\] La determinación de este rango requiere \\(\\lfloor\\log_2{\\lfloor\\log_2{p+1}\\rfloor+1}\\rfloor+1\\) comparaciones; sin embargo, este rango seguramente será muy grande, por el tamaño de los saltos que se estan dando entre puntos de comparación, por lo que no conviene usar busqueda binaria y podemos aplicar \\(B_1\\) para resolver en ese rango acotado.\n\\[\\begin{align}\nF_2(n) &= 2^{2^1}, 2^{2^2}, \\cdots, 2^{2^{\\lfloor \\log_2 {\\lfloor \\log_2 n \\rfloor} + 1 \\rfloor + 1}};\\\\\nC_2(p) &= \\lfloor \\log_2 {\\lfloor \\log_2 p \\rfloor} + 1 \\rfloor + 1 + C_1(p') \\\\\n       &&lt; \\log_2 p + 2\\log_2{\\log_2 p} + O(1);\\\\\n\\end{align}\\] donde \\(p' = p - 2^{2^{\\lfloor \\log_2 {\\lfloor \\log_2 p \\rfloor} + 1 \\rfloor}}\\), es decir, la posición de inserción en el rango ya acotado.\nNote como el término de mayor peso es muy similar a \\(B_1\\) pero destaca la inclusión del término \\(\\log\\log\\) que permite adaptarse a \\(p\\) muy grandes con un pequeño costo adicional, que en términos prácticos se puede ver como una constante.\nLa idea principal es como sigue: una vez determinado el rango, en lugar de usar búsqueda binaria y tener un costo \\(\\lfloor \\log_2 {\\lfloor \\log_2 p \\rfloor} + 1 \\rfloor + 1 + C_\\text{bin}(2^{2^{\\lfloor \\log_2 {\\lfloor \\log_2 p \\rfloor} + 1 \\rfloor + 1}} - 2^{2^{\\lfloor \\log_2 {\\lfloor \\log_2 p \\rfloor} + 1 \\rfloor}})\\) es preferible usar \\(B_1\\) y conseguir un algoritmo que se adapte a la entrada. De manera más precisa, tomar ventaja de \\[C_1(2^{2^{\\lfloor \\log_2 {\\lfloor \\log_2 p \\rfloor} + 1 \\rfloor + 1}} - 2^{2^{\\lfloor \\log_2 {\\lfloor \\log_2 p \\rfloor} + 1 \\rfloor}}) &lt; C_\\text{bin}(2^{2^{\\lfloor \\log_2 {\\lfloor \\log_2 p \\rfloor} + 1 \\rfloor + 1}} - 2^{2^{\\lfloor \\log_2 {\\lfloor \\log_2 p \\rfloor} + 1 \\rfloor}})\\] cuando \\[p' &lt; \\sqrt{2^{2^{\\lfloor \\log_2 {\\lfloor \\log_2 p \\rfloor} + 1 \\rfloor + 1}} - 2^{2^{\\lfloor \\log_2 {\\lfloor \\log_2 p \\rfloor} + 1 \\rfloor}}}\\].\n\nSimplificando las expresiones, la relación que nos describe cuando es mejor usar \\(B_2\\) que la búsqueda binaria es como sigue:\n\\[\\begin{align}\n{\\log_2{p}} + 2\\log_2{\\log_2{p}} &&lt; \\log_2 n\\\\\n2^{\\log_2{p} + \\log_2{\\log^2_2{p}}} &&lt; 2^{\\log_2 {n}}\\\\\n2^{\\log_2{(p \\log^2_2{p})}} &&lt; 2^{\\log_2 {n}}\\\\\n2p \\log_2{p} & &lt; n\\\\\np \\log_2{p^2} & &lt; n\\\\\n\\end{align}\\]\nSi \\(p = \\sqrt{n}\\) entonces \\(\\sqrt{n} \\log_2 n\\) claramente es menor que \\(n\\) incluso para valores relativamente pequeños de \\(n\\), por lo que \\(B_2\\) funciona mejor para \\(p\\) relativamente grandes en comparación con \\(B_1\\).\n\n\n5.2.4 Algoritmo \\(B_k\\)\nBentley y Yao (1976) generalizan la estrategía para cualquier \\(k\\). De manera simplificada:\n\n\\(F_k(n) = 2^{\\cdot^{\\cdot^{\\cdot^{2^i}}}}\\) (exponenciando \\(k\\) veces) para \\(i\\) desde \\(1\\) a \\(\\log_2^{(k)}{n};\\)\n\\(C_k(p) = \\log_2^{(k)}(p) + C_{k-1}(2^{{\\cdot^{\\cdot^{\\cdot^{2^{\\log_2^{(k)}(p)}}}}}} - 2^{{\\cdot^{\\cdot^{\\cdot^{2^{\\log_2^{(k)}(p)-1}}}}}});\\)\n\ndonde \\(\\log_2^{(k)}(n) = \\log_2(\\lfloor \\log_2^{(k-1)}{(n)} \\rfloor + 1)\\), con el caso base de \\(\\log_2^{(1)} n = \\lfloor \\log_2 n \\rfloor + 1\\).\nLa estrategia lleva a que el valor casi óptimo para la búsqueda por comparación se da cuando \\(k=\\log^\\star_2{n}\\) donde \\(\\log^\\star_2\\) es el logaritmo iterado, que esta definido como las veces que se debe iterar aplicando el logaritmo para obtener un valor de \\(1\\) o menor que \\(1\\), i.e., la \\(k\\) más pequeña tal que \\(\\log_2^{(k)}n \\leq 1\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Algoritmos de búsqueda en el modelo de comparación</span>"
    ]
  },
  {
    "objectID": "cap5-busqueda.html#ejercicios",
    "href": "cap5-busqueda.html#ejercicios",
    "title": "5  Algoritmos de búsqueda en el modelo de comparación",
    "section": "5.3 Ejercicios",
    "text": "5.3 Ejercicios\n\nImplementar y probar \\(B_2\\).\nDerivar el costo \\(C_2(p)\\).\n¿Cuando \\(B_1\\) es mejor que \\(B_2\\)?\nHaga un pseudo-código para \\(B_k\\).\n¿Cuál es el costo \\(C_k\\)?\n¿Qué es un árbol binario de búsqueda?\n¿Cuál es el costo de búsqueda en un árbol? ¿qué se debe hacer para asegurar los costos?\n¿Qué es un finger tree?\n¿Cuál es el costo de búsqueda de la skip list?\n¿Cómo se puede hacer la skip list adaptativa? ¿qué otra forma podría aplicar?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Algoritmos de búsqueda en el modelo de comparación</span>"
    ]
  },
  {
    "objectID": "cap5-busqueda.html#material-audio-visual",
    "href": "cap5-busqueda.html#material-audio-visual",
    "title": "5  Algoritmos de búsqueda en el modelo de comparación",
    "section": "5.4 Material audio-visual",
    "text": "5.4 Material audio-visual\nEn el siguiente video se adentraran en diferentes estrategías de búsqueda, notoriamente aquellas que llamaremos oportunistas o adaptables (adaptative). Estas técnicas nos permitirán tomar provecho de instancias sencillas de problemas e incrementar el desempeño en ese tipo de instancias.\nTenga en cuenta que, honrando la literatura, usaremos de forma indiscriminada listas ordenadas como sinónimo de arreglos ordenados.\n\n\n\n\n\n\n\n\nBentley, Jon Louis, y Andrew Chi-Chih Yao. 1976. “An almost optimal algorithm for unbounded searching”. Information processing letters 5 (SLAC-PUB-1679).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Algoritmos de búsqueda en el modelo de comparación</span>"
    ]
  },
  {
    "objectID": "cap6-intersecciones.html",
    "href": "cap6-intersecciones.html",
    "title": "6  Algoritmos de intersección y unión de conjuntos en el modelo de comparación",
    "section": "",
    "text": "Objetivo\nAnalizar el rendimiento de algoritmos de unión e intersección de conjuntos representados como listas ordenadas parametrizando los algoritmos con los algoritmos internos de búsqueda, tamaño de los conjuntos y la distribución de los elementos, bajo un enfoque experimental midiendo los costos en términos del tiempo de ejecución y el uso de memoria.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Algoritmos de intersección y unión de conjuntos en el modelo de comparación</span>"
    ]
  },
  {
    "objectID": "cap6-intersecciones.html#problema",
    "href": "cap6-intersecciones.html#problema",
    "title": "6  Algoritmos de intersección y unión de conjuntos en el modelo de comparación",
    "section": "6.1 Problema",
    "text": "6.1 Problema\nCómo se vió en Capítulos anteriores, un conjunto es una colección de elementos donde no hay repetición. El uso de conjuntos es fundamental para un gran número de problemas. En particular, en este capítulo representaremos conjuntos como arreglos ordenados de números enteros; esto para posicionarlo dentro de un dominio de aplicación objetivo, que es la Recuperación de Información, como parte de la representación de una la matriz dispersa muy grande, llamada índice invertido.\nEstaremos resolviendo los problemas de intersección y unión de conjuntos. Demaine, López-Ortiz, y Munro (2000) demuestra que el costo y procedimiento de las intersecciones y uniones de conjuntos representados como arreglos ordenados, es básicamente el mismo; ya que requieren determinar la misma información. Claramente, colectar los datos para la unión y la intersección, requieren diferentes esfuerzos.\n\n6.1.1 Costo del problema\nEn general, dados dos conjuntos \\(A[1..m] = \\{a_1 &lt; a_2 &lt; \\cdots &lt; a_m \\}\\) y \\(B[1..n] = \\{b_1 &lt; b_2 &lt; \\cdots &lt; b_n \\}\\), el costo de unión es \\[\\log{m+n \\choose m},\\] ver Hwang y Lin (1971).\nDe manera más detallada, supongamos que \\(A \\cap B = \\emptyset\\), esto es, el conjunto de salida será de tamaño \\(m+n\\). De manera similar al razonamiento que se utilizó para el problema de ordenamiento, el problema puede verse como todas las posibles instancias de ordenes o permutaciones de tamaño \\(n+m\\); removiendo la necesidad de los ordenes parciales, esto es \\({n+m \\choose m}\\) posibles instancias de tamaño \\(n+m\\), generadas por dos conjuntos de tamaño \\(n\\) y \\(m\\). Dado que estamos en un modelo basado en comparaciones, y dado el mejor algoritmo \\(s\\) puede dividir el espacio de posibles ordenes en 2, por tanto, dicho algoritmo necesitará \\[\\log_2 {n+m \\choose m}\\] comparaciones para resolver la unión de cualquier par de conjuntos de tamaño \\(m\\) y \\(n\\).\nUsando la aproximación de Stirling para coefficientes binomiales de MacKay (2003), el costo se convierte en: \\[\\log{m+n \\choose m} = n\\log{\\frac{m+n}{n}} + m \\log\\frac{n+m}{m} \\]\n\n\nRecuerde que \\(\\log_2 x = \\frac{\\log_e x}{\\log e}\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Algoritmos de intersección y unión de conjuntos en el modelo de comparación</span>"
    ]
  },
  {
    "objectID": "cap6-intersecciones.html#algoritmos",
    "href": "cap6-intersecciones.html#algoritmos",
    "title": "6  Algoritmos de intersección y unión de conjuntos en el modelo de comparación",
    "section": "6.2 Algoritmos",
    "text": "6.2 Algoritmos\nSe puede observar que si \\(m \\approx n\\), entonces el costo se convierte en \\(O(m + n)\\), esto es, lo más eficiente sería tomar el siguiente algoritmo:\n\n\nfunction merge2!(C, A, B)\n    i = j = 1\n    m, n = length(A), length(B)\n    \n    @inbounds while i &lt;= m ||  j &lt;= n\n        a, b = A[i], B[j]\n        if a == b\n            push!(C, a)\n            i += 1\n            j += 1\n        elseif a &lt; b\n            push!(C, a)\n            i += 1\n        else\n            push!(C, b)\n            j += 1\n        end\n    end\n\n    C\nend\n\nmerge2! (generic function with 1 method)\n\n\n\n6.2.1 Ejercicio\n\nEscriba y pruebe el algoritmo de intersección de merge para \\(n \\approx m\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Algoritmos de intersección y unión de conjuntos en el modelo de comparación</span>"
    ]
  },
  {
    "objectID": "cap6-intersecciones.html#algoritmos-para-arreglos-de-tamaño-muy-diferente",
    "href": "cap6-intersecciones.html#algoritmos-para-arreglos-de-tamaño-muy-diferente",
    "title": "6  Algoritmos de intersección y unión de conjuntos en el modelo de comparación",
    "section": "6.3 Algoritmos para arreglos de tamaño muy diferente",
    "text": "6.3 Algoritmos para arreglos de tamaño muy diferente\nSi \\(m \\ll n\\), el costo tenderá a \\(O(m \\log n)\\), por lo que se pueden realizar \\(m\\) búsquedas binarias directas para localizar la posición de inserción en \\(B\\).\n\n\n\n\n\n\n\n\nlista\n\n\nA\n\n\n\nB\n\n\n\n\na1\n\n1\n\n\n\na2\n\n2\n\n\n\n\na3\n\n3\n\n\n\n\na4\n\n4\n\n\n\n\na5\n\n5\n\n\n\n\na6\n\n6\n\n\n\n\na7\n\n7\n\n\n\n\na8\n\n8\n\n\n\n\na9\n\n9\n\n\n\n\na10\n\n10\n\n\n\n\na11\n\n11\n\n\n\n\na12\n\n12\n\n\n\n\na13\n\n13\n\n\n\n\na15\n\n15\n\n\n\n\na16\n\n16\n\n\n\n\n1\n\n1\n\n\n\n2\n\n2\n\n\n\n\n3\n\n3\n\n\n\n\n4\n\n4\n\n\n\n\n5\n\n5\n\n\n\n\n6\n\n6\n\n\n\n\n7\n\n7\n\n\n\n\n8\n\n8\n\n\n\n\n9\n\n9\n\n\n\n\n10\n\n10\n\n\n\n\n11\n\n11\n\n\n\n\n12\n\n12\n\n\n\n\n13\n\n13\n\n\n\n\n15\n\n15\n\n\n\n\n16\n\n16\n\n\n\n\n\n\n\nFigura 6.1: Dos listas alineadas donde los nodos sombreados son elementos de los conjuntos.\n\n\n\n\n\nSe hace notar que \\(A\\) y \\(B\\) estan ordenados, y por lo tanto, localizar \\(A[i]\\) en \\(B[j]\\) significa que \\(B[j-1] &lt; A[i]\\), por lo que intentar localizar \\(A[i+1]\\) puede comenzar en \\(B[j+1]\\). A continuación se muestra el código de un algoritmo de intersección usando algoritmos de búsqueda con memoria de la posición anterior.\n\nfunction intsearch!(C, A, B, algosearch=doublingsearch)\n    if length(B) &lt; length(A)\n        A, B = B, A\n    end\n\n    p = 1\n    for (i, a) in enumerate(A)\n        p = algosearch(B, a, p)\n        p &gt; length(B) && break\n        if a == B[p]\n            p += 1\n            push!(C, a)\n        end\n    end\n\n    C\nend\n\nHwang y Lin (1971) propone otro algoritmo que funciona para casos similares:\n\nDivide \\(B\\) en bloques de tamaño \\(m\\), define un arreglo virtual \\(B'[1..n/m]\\) donde \\(B'[i] = B[i \\cdot m]\\)\nSe búsca la posición de inserción \\(p\\) de cada \\(a \\in A\\) en \\(B'\\), costando \\(\\log n/m\\) para cada búsqueda.\nDespués se localiza dentro del \\(B\\) en el \\(p\\)-ésimo bloque, i.e., \\(B[(p-1)m + 1 .. p\\cdot m]\\), por la posición de inserción del bloque, con un costo de \\(\\log m\\).\n\nEntonces, se obtiene un costo de \\(O(m \\log{n/m} + m \\log m)\\); esto es equivalente en el peor caso a búsquedas directas, i.e., las posiciones de inserción de \\(a \\in A\\) se encuentran distribuidas de manera uniforme en \\(B\\). Sin embargo, es posible mejorar si se descartan bloques en el paso 1. Esto es, si se hay concentración de elementos de \\(A\\) en bloques de \\(B\\). Para esto, es necesario un análisis de costo promedio, el cual se muestra en el artículo.\nIncluso cuando hay concentración, podemos recordar la \\((i-1)\\) posición de inserción para iniciar la \\(i\\)-ésima búsqueda, y sacar provecho de posiciones esperadas cercanas de la posición inicial de búsqueda, i.e., podemos utilizar algoritmos de búsqueda adaptables para mejorar el desempeño.\n\n6.3.1 Algoritmo de Baeza Yates\nBaeza-Yates (2004) propone un algoritmo eficiente para intersecciones de dos conjuntos. El algoritmo tiene una estrategía dividir para vencer:\n\nSe toma la mediana \\(M\\) de \\(A\\) y se busca en \\(B\\) obteniendo su posición de inserción \\(p\\).\nEl problema entonces se divide en 3 subproblemas: \\[\\begin{align}\nC_&lt; &= \\{A[1..M-1] \\cap B[1..p-e]\\} \\\\\nC_= &= \\{A[M]\\} \\cap \\{B[p]\\} \\\\\nC_&gt; &= \\{A[M+1..m] \\cap B[p+e..n]\\} \\\\\n\\end{align}\\] donde \\(e=1\\) si \\(A[M] = B[p]\\) y \\(e=0\\) cuando \\(A[M] \\not= B[p]\\).\nLa unión de estos tres conjuntos es la solución \\(C_&lt; \\cup C_= \\cup C_&gt;\\).\nEl problema \\(C_=\\) es trivial, y \\(C_&lt;\\) y \\(C_&gt;\\) se implementan recurriendo, ajustando los rangos de trabajo.\n\nA continuación se muestra el código en Julia, usando los algoritmos de búsqueda del Cap. 5.\n\n# Adaptado de https://github.com/sadit/Intersections.jl\n\n1function baezayates!(output, A, B, findpos::Function=binarysearch)\n    baezayates!(output, A, 1, length(A), B, 1, length(B), findpos)\nend\n\nfunction baezayates!(output, A, a_sp::Int, a_ep::Int, B, b_sp::Int, b_ep::Int, findpos::Function)\n    (a_ep &lt; a_sp || b_ep &lt; b_sp) && return output\n    imedian = ceil(Int, (a_ep + a_sp) / 2)\n    median = A[imedian]\n    ## our findpos returns n + 1 when median is larger than B[end]\n2    medpos = min(findpos(B, median, b_sp), b_ep)\n    \n    matches = median == B[medpos] \n3    baezayates!(output, A, a_sp, imedian - 1, B, b_sp, medpos - matches, findpos)\n4    matches && push!(output, median)\n5    baezayates!(output, A, imedian + 1, a_ep, B, medpos + matches, b_ep, findpos)\n    output\nend\n\n\n1\n\nPunto de entrada.\n\n2\n\nBúsqueda de la posición de inserción de la mediana de \\(A\\) en \\(B\\).\n\n3\n\nRecurrencia para el problema \\(C_&lt;\\).\n\n4\n\nAñadir al resultado el valor de la mediana si es que se encontró en \\(B\\); es importante que este paso este entre las recurrencias para que output sea un arreglo ordenado.\n\n5\n\nRecurencia para el problema \\(C_&gt;\\).\n\n\n\n\nEl algoritmo de Baeza Yates es óptimo en el peor caso y es capaz de aprovechar casos donde \\(C_&lt;\\) o \\(C_&gt;\\) se convierten en triviales, lo cual da muy buenos casos en algunas distribuciones.\n\n\n6.3.2 Ejercicios\n\nImplemente la unión con el algoritmo de Baeza Yates.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Algoritmos de intersección y unión de conjuntos en el modelo de comparación</span>"
    ]
  },
  {
    "objectID": "cap6-intersecciones.html#operaciones-con-tres-o-más-conjuntos",
    "href": "cap6-intersecciones.html#operaciones-con-tres-o-más-conjuntos",
    "title": "6  Algoritmos de intersección y unión de conjuntos en el modelo de comparación",
    "section": "6.4 Operaciones con tres o más conjuntos",
    "text": "6.4 Operaciones con tres o más conjuntos\nLos algoritmos y costos hasta ahora revisados se cumplen para dos conjuntos; se mencionaron diferentes algoritmos, algunos de ellos especializados por características como las proporciones de los conjuntos de entrada.\nEn particular, es importante hacer notar que ni el problema ni las aplicaciones estan limitadas a dos conjuntos, y por tanto, es importante algoritmos y estrategías para resolver \\(\\bigcup_i A_i\\) así como \\(\\bigcap_i A_i\\).\n\n6.4.1 Algoritmo SvS\nDado \\(C = A \\cap B\\) es un hecho que \\(|C| \\leq min \\{|A|, |B|\\}\\). Recordando, que hay maneras relativamente simples y eficientes de resolver la intersección cuando \\(m \\ll n\\); por tanto, cuando tenemos más de dos conjuntos podemos aplicar la estrategía Small vs Small (SvS), que consisten en intersectar los \\(k\\) conjuntos por pares intersectando el par de arreglos más pequeños cada vez.\n\n# Adaptado de https://github.com/sadit/Intersections.jl\n\nfunction svs(L::Vector{T}, in2::Function=baezayates!) where T\n    prev, curr = eltype(T)[], eltype(T)[]\n    sort!(L, by=length, rev=true)\n    curr = pop!(L)\n\n    while length(L) &gt; 0\n        empty!(prev)\n        isize = in2(prev, curr, pop!(L))\n        isize == 0 && return prev\n        prev, curr = curr, prev\n    end\n\n    curr\nend\n\n\n\n6.4.2 Algoritmo de Barbay y Kenyon\nExiste otra familia de algoritmos, basados en búsquedas adaptativas que pueden llegar a mejorar el desempeño bajo cierto tipo de entradas. Demaine, López-Ortiz, y Ian Munro (2001), Barbay, López-Ortiz, y Lu (2006), y Barbay et al. (2010) muestran algoritmos de intersección basados en búsqueda adaptables para aprovechar instancias simples. Estos estudios se basan en contribuciones teóricas de los mismos autores: Demaine, López-Ortiz, y Munro (2000), Demaine, López-Ortiz, y Ian Munro (2001), Barbay y Kenyon (2002) y Baeza-Yates (2004).\nEl algoritmo de Barbay, López-Ortiz, y Lu (2006) trabaja sobre los \\(k\\) conjuntos de entrada, representados como arreglos ordenados de números enteros. Es un algoritmo simple pero poderoso: hace uso de búsquedas adaptivas con memoria para guardar las posiciones donde se avanza, de tal forma que no se recalculen posiciones. Las diferentes estrategias para revisar los conjuntos pueden dar diferentes desempeños, como se valida en Barbay et al. (2010), donde además de hacer una gran variedad de experimentos sobre diferentes algoritmos de búsqueda, se introducen variantes en el orden de acceso de cada conjunto.\nA continuación se muestra el código del algoritmo base:\n\n# Adaptado de https://github.com/sadit/Intersections.jl\n\nfunction bk!(output, L::AbstractVector, findpos::Function=doublingsearch)\n    P = ones(Int, length(L))\n    bk!(output, L, P, findpos)\nend\n \n1function bk!(output, L, P, findpos::Function=doublingsearch)\n2    n = length(L)\n3    el = L[1][1]\n4    c = 0\n\n    @inbounds while true\n        for i in eachindex(P)\n5            P[i] = findpos(L[i], el, P[i])\n            P[i] &gt; length(L[i]) && return output\n            pval = L[i][P[i]]\n            if pval == el\n                c += 1\n6                if c == n\n                    push!(output, el)\n7                    c = 0\n                    P[i] += 1\n                    P[i] &gt; length(L[i]) && return output\n                    el = L[i][P[i]]\n                end\n            else\n                c = 0\n                el = pval\n            end\n        end\n    end\n\n    output\nend\n\n\n1\n\nEl algoritmo de Barbay & Kenyon recibe: i) output el conjunto de salida. ii) L la lista de conjuntos (representados como arreglos ordenados). iii) P arreglo de posiciones actuales para cada arreglo. iv) findpos` función de búsqueda.\n\n2\n\nNúmero de conjuntos en L.\n\n3\n\nel es el elemento siendo búscado en todos los arreglos.\n\n4\n\nc número de listas que contienen el.\n\n5\n\nBúscando la posición de inserción de el en L[i], comenzando en P[i].\n\n6\n\nEsta igualdad implica que hay interección.\n\n7\n\nReiniciando el y c y actualizando P[i].\n\n\n\n\nDe manera particular, Barbay et al. (2010) presentan un estudio experimental sobre los algoritmos presentados en el área durante la decada de 2000 a 2010, dichos algoritmos se parametrizaron de maneras que nos permiten aprender diferentes características de cada uno de ellos, dependiendo de los algoritmos de búsqueda que usan, la arquitectura computacional donde se evalúa, y el número de conjuntos siendo procesados.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Algoritmos de intersección y unión de conjuntos en el modelo de comparación</span>"
    ]
  },
  {
    "objectID": "cap6-intersecciones.html#recursos-audio-visuales-de-la-unidad",
    "href": "cap6-intersecciones.html#recursos-audio-visuales-de-la-unidad",
    "title": "6  Algoritmos de intersección y unión de conjuntos en el modelo de comparación",
    "section": "6.5 Recursos audio-visuales de la unidad",
    "text": "6.5 Recursos audio-visuales de la unidad\nParte 1: Algoritmos de intersección (y unión) de listas ordenadas \nParte 2: Algoritmos de intersección y algunas aplicaciones",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Algoritmos de intersección y unión de conjuntos en el modelo de comparación</span>"
    ]
  },
  {
    "objectID": "cap6-intersecciones.html#actividades",
    "href": "cap6-intersecciones.html#actividades",
    "title": "6  Algoritmos de intersección y unión de conjuntos en el modelo de comparación",
    "section": "6.6 Actividades",
    "text": "6.6 Actividades\nImplementación y comparación de diferentes algoritmos de intersección de conjuntos.\nLea cuidadosamente las instrucciones y desarrolle las actividades. Entregue el reporte correspondiente en tiempo.\n\n\n\n\n\n\nBaeza-Yates, Ricardo. 2004. “A fast set intersection algorithm for sorted sequences”. En Combinatorial Pattern Matching: 15th Annual Symposium, CPM 2004, Istanbul, Turkey, July 5-7, 2004. Proceedings 15, 400–408. Springer.\n\n\nBarbay, Jérémy, y Claire Kenyon. 2002. “Adaptive intersection and t-threshold problems”. En Proceedings of the Thirteenth Annual ACM-SIAM Symposium on Discrete Algorithms, 390–99. SODA ’02. USA: Society for Industrial; Applied Mathematics.\n\n\nBarbay, Jérémy, Alejandro López-Ortiz, y Tyler Lu. 2006. “Faster adaptive set intersections for text searching”. En Experimental Algorithms: 5th International Workshop, WEA 2006, Cala Galdana, Menorca, Spain, May 24-27, 2006. Proceedings 5, 146–57. Springer.\n\n\nBarbay, Jérémy, Alejandro López-Ortiz, Tyler Lu, y Alejandro Salinger. 2010. “An experimental investigation of set intersection algorithms for text searching”. Journal of Experimental Algorithmics (JEA) 14: 3–7.\n\n\nDemaine, Erik D, Alejandro López-Ortiz, y J Ian Munro. 2001. “Experiments on adaptive set intersections for text retrieval systems”. En Algorithm Engineering and Experimentation: Third International Workshop, ALENEX 2001 Washington, DC, USA, January 5–6, 2001 Revised Papers 3, 91–104. Springer.\n\n\nDemaine, Erik D, Alejandro López-Ortiz, y J Ian Munro. 2000. “Adaptive set intersections, unions, and differences”. En Proceedings of the eleventh annual ACM-SIAM symposium on Discrete algorithms, 743–52.\n\n\nHwang, Frank K., y Shen Lin. 1971. “Optimal merging of 2 elements with n elements”. Acta Informatica 1 (2): 145–58.\n\n\nMacKay, David JC. 2003. Information theory, inference and learning algorithms. Cambridge university press.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Algoritmos de intersección y unión de conjuntos en el modelo de comparación</span>"
    ]
  },
  {
    "objectID": "refs.html",
    "href": "refs.html",
    "title": "References",
    "section": "",
    "text": "Baeza-Yates, Ricardo. 2004. “A Fast Set Intersection Algorithm for\nSorted Sequences.” In Combinatorial Pattern Matching: 15th\nAnnual Symposium, CPM 2004, Istanbul, Turkey, July 5-7, 2004.\nProceedings 15, 400–408. Springer.\n\n\nBarbay, Jérémy, and Claire Kenyon. 2002. “Adaptive Intersection\nand t-Threshold Problems.” In Proceedings of the Thirteenth\nAnnual ACM-SIAM Symposium on Discrete Algorithms, 390–99. SODA ’02.\nUSA: Society for Industrial; Applied Mathematics.\n\n\nBarbay, Jérémy, Alejandro López-Ortiz, and Tyler Lu. 2006. “Faster\nAdaptive Set Intersections for Text Searching.” In\nExperimental Algorithms: 5th International Workshop, WEA 2006, Cala\nGaldana, Menorca, Spain, May 24-27, 2006. Proceedings 5, 146–57.\nSpringer.\n\n\nBarbay, Jérémy, Alejandro López-Ortiz, Tyler Lu, and Alejandro Salinger.\n2010. “An Experimental Investigation of Set Intersection\nAlgorithms for Text Searching.” Journal of Experimental\nAlgorithmics (JEA) 14: 3–7.\n\n\nBentley, Jon Louis, and Andrew Chi-Chih Yao. 1976. “An Almost\nOptimal Algorithm for Unbounded Searching.” Information\nProcessing Letters 5 (SLAC-PUB-1679).\n\n\nCook, Curtis R, and Do Jin Kim. 1980. “Best Sorting Algorithm for\nNearly Sorted Lists.” Communications of the ACM 23 (11):\n620–24.\n\n\nCormen, Thomas H, Charles E Leiserson, Ronald L Rivest, and Clifford\nStein. 2022. Introduction to Algorithms. MIT press.\n\n\nDemaine, Erik D, Alejandro López-Ortiz, and J Ian Munro. 2001.\n“Experiments on Adaptive Set Intersections for Text Retrieval\nSystems.” In Algorithm Engineering and Experimentation: Third\nInternational Workshop, ALENEX 2001 Washington, DC, USA, January 5–6,\n2001 Revised Papers 3, 91–104. Springer.\n\n\nDemaine, Erik D, Alejandro López-Ortiz, and J Ian Munro. 2000.\n“Adaptive Set Intersections, Unions, and Differences.” In\nProceedings of the Eleventh Annual ACM-SIAM Symposium on Discrete\nAlgorithms, 743–52.\n\n\nEstivill-Castro, Vladmir, and Derick Wood. 1992. “A Survey of\nAdaptive Sorting Algorithms.” ACM Computing Surveys\n(CSUR) 24 (4): 441–76.\n\n\nHwang, Frank K., and Shen Lin. 1971. “Optimal Merging of 2\nElements with n Elements.” Acta Informatica 1 (2):\n145–58.\n\n\nKnuth, Donald. 1998. The Art of Computer Programming, Vol. 3 (2nd\nEd): Sorting and Searching. Vol. 3. Redwood City, CA, USA.: Addison\nWesley Longman Publishing Co. Inc.\n\n\nLoeser, Rudolf. 1974. “Some Performance Tests of\n‘Quicksort’ and Descendants.” Communications of\nthe ACM 17 (3): 143–52.\n\n\nMacKay, David JC. 2003. Information Theory, Inference and Learning\nAlgorithms. Cambridge university press.\n\n\nPugh, William. 1990. “Skip Lists: A Probabilistic Alternative to\nBalanced Trees.” Commun. ACM 33 (6): 668–76. https://doi.org/10.1145/78973.78977.\n\n\nScott, Jennifer, and Miroslav Tůma. 2023. “An Introduction to\nSparse Matrices.” In Algorithms for Sparse Linear\nSystems, 1–18. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-25820-6_1.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "galeria-actividades.html",
    "href": "galeria-actividades.html",
    "title": "Galería de actividades MCDI",
    "section": "",
    "text": "Semestre 2025-1\nEste curso se impartió en este formato en MCDI-2025-1; el plan es modificar las actividades ligeramente con forme pasan los semestres y que la galería apoye a los nuevos estudiantes para tener ejemplos claros de lo que se solicita.",
    "crumbs": [
      "Apéndices",
      "Galería de actividades MCDI"
    ]
  },
  {
    "objectID": "galeria-actividades.html#semestre-2025-1",
    "href": "galeria-actividades.html#semestre-2025-1",
    "title": "Galería de actividades MCDI",
    "section": "",
    "text": "Alumno\nSitio\n\n\n\n\nAlma Diana Herrera Ortiz\nhttps://aldihero.github.io/datascience_foundations/\n\n\nArif Narváez de la O\nhttps://arifnvz.github.io/about.html\n\n\nBrigitte Rarinka Godinez Montoya\nhttps://github.com/Programadari/analisis-de-algoritmos-2025-1/\n\n\nDavid Segundo Garcia\nhttps://davidsg24.github.io/An-lisis-de-algoritmos/\n\n\nIsaac Hernández Ramírez\nhttps://isaachr141522.github.io/reportes_algoritmos/\n\n\nJosé Alberto Villegas Díaz Disciplina\nhttps://albertovillegas07.github.io/AnalisisAlgoritmos2025/\n\n\nJosé Francisco Cázarez Marroquín\nhttps://dsfrankcaza.github.io/Analsis-de-Algoritmos/\n\n\nJuan Antonio Velasquez Martinez\nhttps://juan21javm.github.io/proyecto-analisis-de-algoritmos/\n\n\nLuis Alberto Rodríguez Catana\nhttps://albertocat.github.io/Algoritmos/\n\n\nSantiago Botero Sierra\nhttps://sboteros.com/about.html",
    "crumbs": [
      "Apéndices",
      "Galería de actividades MCDI"
    ]
  }
]